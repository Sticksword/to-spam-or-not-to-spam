{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# good pre-reads\n",
    "# paper that came with enron data: http://www2.aueb.gr/users/ion/docs/ceas2006_paper.pdf\n",
    "\n",
    "import os\n",
    "import re\n",
    "from typing import Any\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import urlextract\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, TfidfTransformer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn import model_selection\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.metrics import precision_score, recall_score, classification_report, confusion_matrix\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amount of ham files: 3672\n",
      "Amount of spam files: 1500\n",
      "Spam to Ham Ratio: 0.4084967320261438\n"
     ]
    }
   ],
   "source": [
    "ham_filenames = [name for name in sorted(os.listdir('data/enron1/ham')) if len(name) > 15]\n",
    "spam_filenames = [name for name in sorted(os.listdir('data/enron1/spam')) if len(name) > 15]\n",
    "\n",
    "print('Amount of ham files:', len(ham_filenames))\n",
    "print('Amount of spam files:', len(spam_filenames))    \n",
    "print('Spam to Ham Ratio:',len(spam_filenames)/len(ham_filenames))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Subject: our recommendations make you big money\\n', 'c . d . financial newsletter\\n', 'alert :\\n', 'strong buy\\n', 'issued on dtoi\\n', 'news alert * * * * * * * * 3 / 25 / 2004 4 : 00 pm est\\n', '* dtomi receives request for quote for tandem axle\\n', 'enclosed trailers *\\n', '* dtoi up 50 % *\\n', 'dtomi , inc .\\n', 'symbol : dtoi otc . bb\\n', 'price $ 1 . 25\\n', 'shares out : 26 million\\n', 'market capitalization : $ 24 million\\n', 'significant revenue growth in 2004\\n', 'rating : strong buy\\n', '7 days trading target : $ 2 . 00\\n', '30 day trading target : $ 3 . 50\\n', '* * strong buy alert * * strong\\n', 'buy alert * *\\n', 'dtomi , inc . ( dtomi or the\\n', 'company ) markets a unique air spring suspension system , air spring axlet\\n', ', that allows small to medium - sized trailers to be lowered to near flush with\\n', 'the ground , greatly improving the safety and ease with which heavy cargo can be\\n', 'loaded and unloaded . an aggressive commercialization schedule may bring\\n', 'profitability by mid 2004 .\\n', 'dtomi , inc is about to\\n', 'release big news in the next week and the stock is expected to rise through the\\n', 'roof . as one of our subscribers you already probably reaped the benefits of our\\n', 'china world trade corp ( cwtd ) recommendation 2 weeks ago . cwtd rose 900 % in\\n', \"only 3 days and all of our subscribers turned a quick buck ! don ' t delay\\n\", 'and get in on the action early ! ! dtomi , inc . ( dtoi ) is about to explode so get\\n', \"in on the action before it ' s too late !\\n\", '·\\n', 'dtomi , inc . ( dtomi or the\\n', 'company ) markets a unique air spring suspension system , airspring axle , that\\n', 'allows small to medium - sized trailers to be lowered to near flush with the\\n', 'ground , greatly improving the safety and ease with which heavy cargo can be\\n', 'loaded and unloaded\\n', '·\\n', 'over 2 million small to medium\\n', 'sized trailers are sold annually in the us .\\n', 'these trailers are priced up to and over $ 5 , 000 ,\\n', 'and are marketed to rental agencies , home - improvement retailers that maintain\\n', 'rental fleets , and individuals who need to haul heavy items such as contractor\\n', 'equipment , motorcycles , snowmobiles , jet skis , cars and horses .\\n', '·\\n', 'management estimates that the\\n', \"initial market for the company ' s air spring axle system is $ 10 - $ 15 million .\\n\", 'there are thousands of trailer rental locations in the us , and the system can\\n', 'improve customer satisfaction , safety and labor requirements . dtomi is working\\n', 'with a leading home improvement conducting a four - month test of 50 trailers .\\n', '·\\n', 'the total market will grow to\\n', '$ 140 million as the system is licensed to trailer manufacturers .\\n', 'dtomi plans to assist manufacturers with\\n', 'integration of the air spring axle system into existing trailer designs , and to\\n', 'collaborate with them on marketing efforts . management expects licensing fees of\\n', '$ 200 - $ 300 per trailer .\\n', '·\\n', 'dtomi expects be cash flow\\n', 'positive by the end of 2004 . an\\n', 'aggressive multi - media marketing effort is underway , and rental fleet sales and\\n', 'licensing revenues are expected to bring high margins . the company recently\\n', 'raised approximately $ 1 . 3 million in equity capital .\\n', '·\\n', 'the company has begun\\n', 'aggressively marketing its product\\n', 'through a variety of methods , in an attempt to gain early market share and\\n', 'preempt any competition or patent infringements .\\n', '·\\n', 'these trailers are sold to\\n', 'individual consumers , car and truck rental agencies , and\\n', 'hardware stores that maintain rental fleets . there are several thousand\\n', 'trailer rental locations in the u . s . , including true value hardware ,\\n', 'with 4 , 100 rental locations ; united rentals , with 550 locations ;\\n', 'and home depot , with 520 locations .\\n', '·\\n', 'dtomi\\n', 'estimates that the near - term domestic market for the air spring axlet is 50 , 000\\n', 'units per year , which should generate revenues of $ 10 to $ 15 million .\\n', 'overhead is low , and initial margins should exceed 30 % . commercialization is\\n', 'expected to ramp up very quickly , and the company expects to be cash flow\\n', 'positive by year - end 2004\\n', '·\\n', 'dtomi\\n', 'raised approximately $ 1 . 26 million in equity\\n', 'financing through december 2003 , management expects that this cash will cover\\n', 'all important rollout operations and support the commercialization of the air\\n', 'spring axlet until breakeven is reached .\\n', 'the\\n', 'rental and specialty trailer market\\n', 'the 1992 census indicated that there are\\n', 'approximately 15 million small and medium sized trailers in the u . s . , and that\\n', 'about one million units are sold annually . data from the leading trailer\\n', 'manufacturer , wells cargo , suggests that the market may be two million trailers\\n', 'per year . these trailers are sold to individual consumers , car and truck rental\\n', 'agencies , and hardware stores that maintain rental fleets . there are several\\n', 'thousand trailer rental locations in the u . s . major proprietors include true\\n', 'value hardware , with 4 , 100 rental locations ; united rentals , with 550 locations ;\\n', 'and home depot , with 520 locations .\\n', 'do not waste\\n', \"your money investing in companies that don ' t have what it takes to be\\n\", 'successful . a track record of years of business is just the kind of foundation a\\n', 'great company needs to succeed .\\n', 'disclaimer :\\n', 'verify all claims and do your own due diligence . apple investments , inc profiles\\n', 'are not a solicitation or recommendation to buy , sell or hold securities . apple\\n', 'investments , inc is not offering securities for sale . an offer to buy or sell can\\n', 'be made only with accompanying disclosure documents and only in the states and\\n', 'provinces for which they are approved . all statements and expressions are the\\n', 'sole opinion of the editor and are subject to change without notice . apple\\n', 'investments , inc is not liable for any investment decisions by its readers or\\n', 'subscribers . it is strongly recommended that any purchase or sale decision be\\n', 'discussed with a financial adviser , or a broker - dealer , or a member of any\\n', 'financial regulatory bodies . the information contained in apple investments , inc\\n', 'profiles is provided as an information service only . the accuracy or\\n', 'completeness of the information is not warranted and is only as reliable as the\\n', 'sources from which it was obtained . it should be understood the there is no\\n', 'guarantee past performance will be indicative of future results . investors are\\n', 'cautioned that they may lose all or a portion of their investment if they make a\\n', 'purchase in apple investments , inc profiled stocks . in order to be in full\\n', 'compliance with the securities act of 1933 , section 17 ( b ) , apple investments , inc\\n', 'and its management receives fees from profiled companies or agents representing\\n', 'the profiled companies . these fees may be paid in cash or in stock and they will\\n', 'be fully disclosed in each profile . apple investments , inc has been compensated\\n', 'twenty thousand dollars cash for the dissemination for this report , by a third\\n', 'party . neither apple investments , inc nor any of its affiliates , or employees\\n', 'shall be liable to you or anyone else for any loss or damages from use of this\\n', 'information piece , caused in whole or part by its negligence or contingencies\\n', 'beyond its control in procuring , compiling , interpreting , reporting or\\n', 'delivering this e mail and any contents the reader should verify all claims and\\n', 'do their own due diligence before investing in any securities mentioned .\\n', 'investing in securities is speculative and carries a high degree of risk . the\\n', 'information found in this home page is protected by the copyright laws of the\\n', 'united states and may not be copied , or reproduced in any way without the\\n', 'expressed , written consent of the editor of apple investments , inc .\\n', 'gfymo enxcf gjxsd rmxjx rwtre pwtkkmtyep pkhnh trdoj ktowv ikgra pphzv zzsyt sudlz\\n']\n",
      "\n",
      "\n",
      "\n",
      "what one test email looks like:  {'subject': 'Subject: dobmeos with hgh my energy level has gone up ! stukm\\n', 'body': ['introducing\\n', 'doctor - formulated\\n', 'hgh\\n', 'human growth hormone - also called hgh\\n', 'is referred to in medical science as the master hormone . it is very plentiful\\n', 'when we are young , but near the age of twenty - one our bodies begin to produce\\n', 'less of it . by the time we are forty nearly everyone is deficient in hgh ,\\n', 'and at eighty our production has normally diminished at least 90 - 95 % .\\n', 'advantages of hgh :\\n', '- increased muscle strength\\n', '- loss in body fat\\n', '- increased bone density\\n', '- lower blood pressure\\n', '- quickens wound healing\\n', '- reduces cellulite\\n', '- improved vision\\n', '- wrinkle disappearance\\n', '- increased skin thickness texture\\n', '- increased energy levels\\n', '- improved sleep and emotional stability\\n', '- improved memory and mental alertness\\n', '- increased sexual potency\\n', '- resistance to common illness\\n', '- strengthened heart muscle\\n', '- controlled cholesterol\\n', '- controlled mood swings\\n', '- new hair growth and color restore\\n', 'read\\n', 'more at this website\\n', 'unsubscribe\\n']}\n"
     ]
    }
   ],
   "source": [
    "def load_file(is_spam, filename):\n",
    "    directory = \"data/enron1/spam\" if is_spam else \"data/enron1/ham\"\n",
    "    # http://python-notes.curiousefficiency.org/en/latest/python3/text_file_processing.html\n",
    "    # latin-1 encoding because some of the spam files have retarded characters\n",
    "    with open(os.path.join(directory, filename), encoding=\"latin-1\") as f:\n",
    "        lines = f.readlines()\n",
    "        # like this one\n",
    "        if filename.startswith(\"0754\"):\n",
    "            print(lines)\n",
    "        return {\n",
    "            'subject': lines[0],\n",
    "            'body': lines[1:],\n",
    "        }\n",
    "    \n",
    "ham_emails = [load_file(is_spam=False, filename=name) for name in ham_filenames]\n",
    "spam_emails = [load_file(is_spam=True, filename=name) for name in spam_filenames]\n",
    "    \n",
    "    \n",
    "testEmail = spam_emails[0]\n",
    "\n",
    "print('\\n\\n')\n",
    "print('what one test email looks like: ', testEmail)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "snowball_stemmer = SnowballStemmer('english')\n",
    "\n",
    "class EmailJsonToText(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, lowercaseConversion = True, punctuationRemoval = True, \n",
    "                 urlReplacement = True, numberReplacement = False, stemming = False):\n",
    "        self.lowercaseConversion = lowercaseConversion\n",
    "        self.punctuationRemoval = punctuationRemoval\n",
    "        self.urlReplacement = urlReplacement\n",
    "        self.urlExtractor = urlextract.URLExtract()\n",
    "        self.numberReplacement = numberReplacement\n",
    "        self.stemming = stemming\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X, y=None):\n",
    "        X_to_words = []\n",
    "        for emailJson in X:\n",
    "            text = ' '.join(emailJson['body'])\n",
    "            \n",
    "            if text is None:\n",
    "                text = 'empty'\n",
    "            if self.lowercaseConversion:\n",
    "                text = text.lower()\n",
    "                \n",
    "            if self.urlReplacement:\n",
    "                urls = self.urlExtractor.find_urls(text)\n",
    "                for url in urls:\n",
    "                   text = text.replace(url, 'URL')   \n",
    "            \n",
    "            # apparently removing numbers helped\n",
    "            if self.numberReplacement:\n",
    "                text = re.sub('\\d', '%d', text)\n",
    "                    \n",
    "            if self.punctuationRemoval:\n",
    "                text = text.replace('.','')\n",
    "                text = text.replace(',','')\n",
    "                text = text.replace('!','')\n",
    "                text = text.replace('?','')\n",
    "                \n",
    "            if self.stemming:\n",
    "                words = text.split(' ')\n",
    "                \n",
    "                stemmed_words = []\n",
    "                for word in words:\n",
    "                    stemmed_words.append(snowball_stemmer.stem(word))\n",
    "                \n",
    "                text = ' '.join(stemmed_words)\n",
    "            \n",
    "            X_to_words.append(text)\n",
    "        return np.array(X_to_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<3x173 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 175 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_few = ham_emails[:3]\n",
    "X_few_text = EmailJsonToText().fit_transform(X_few)\n",
    "vocab_transformer = CountVectorizer()\n",
    "X_few_vectors = vocab_transformer.fit_transform(X_few_text)\n",
    "X_few_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'gary': 101,\n",
       " 'production': 139,\n",
       " 'from': 98,\n",
       " 'the': 157,\n",
       " 'high': 111,\n",
       " 'island': 118,\n",
       " 'larger': 119,\n",
       " 'block': 63,\n",
       " 'commenced': 74,\n",
       " 'on': 134,\n",
       " 'saturday': 145,\n",
       " 'at': 57,\n",
       " '00': 0,\n",
       " 'about': 49,\n",
       " '500': 30,\n",
       " 'gross': 107,\n",
       " 'carlos': 70,\n",
       " 'expects': 91,\n",
       " 'between': 61,\n",
       " 'and': 53,\n",
       " '10': 3,\n",
       " '000': 1,\n",
       " 'for': 96,\n",
       " 'tomorrow': 161,\n",
       " 'vastar': 164,\n",
       " 'owns': 136,\n",
       " '68': 36,\n",
       " 'of': 133,\n",
       " 'george': 103,\n",
       " '6992': 37,\n",
       " 'forwarded': 97,\n",
       " 'by': 66,\n",
       " 'weissman': 168,\n",
       " 'hou': 112,\n",
       " 'ect': 85,\n",
       " '12': 6,\n",
       " '13': 7,\n",
       " '99': 48,\n",
       " '16': 9,\n",
       " 'am': 51,\n",
       " 'daren': 80,\n",
       " 'farmer': 92,\n",
       " '38': 26,\n",
       " 'to': 159,\n",
       " 'rodriguez': 144,\n",
       " 'cc': 71,\n",
       " 'melissa': 122,\n",
       " 'graves': 106,\n",
       " 'subject': 152,\n",
       " 'resources': 143,\n",
       " 'inc': 116,\n",
       " 'please': 137,\n",
       " 'call': 67,\n",
       " 'linda': 120,\n",
       " 'get': 104,\n",
       " 'everything': 90,\n",
       " 'set': 147,\n",
       " 'up': 163,\n",
       " 'going': 105,\n",
       " 'estimate': 89,\n",
       " 'coming': 72,\n",
       " 'with': 170,\n",
       " 'increase': 117,\n",
       " 'each': 84,\n",
       " 'following': 95,\n",
       " 'day': 82,\n",
       " 'based': 59,\n",
       " 'my': 125,\n",
       " 'conversations': 77,\n",
       " 'bill': 62,\n",
       " 'fischer': 94,\n",
       " 'bmar': 64,\n",
       " '34': 24,\n",
       " 'enron': 87,\n",
       " 'north': 128,\n",
       " 'america': 52,\n",
       " 'corp': 78,\n",
       " 'bryan': 65,\n",
       " 'darren': 81,\n",
       " 'attached': 58,\n",
       " 'appears': 54,\n",
       " 'be': 60,\n",
       " 'nomination': 126,\n",
       " 'previously': 138,\n",
       " 'erroneously': 88,\n",
       " 'referred': 142,\n",
       " 'as': 56,\n",
       " 'well': 169,\n",
       " 'now': 130,\n",
       " 'commence': 73,\n",
       " 'sometime': 151,\n",
       " 'told': 160,\n",
       " 'harris': 108,\n",
       " 'that': 156,\n",
       " 'we': 167,\n",
       " 'her': 109,\n",
       " 'telephone': 154,\n",
       " 'number': 131,\n",
       " 'in': 115,\n",
       " 'gas': 102,\n",
       " 'control': 76,\n",
       " 'so': 149,\n",
       " 'she': 148,\n",
       " 'can': 69,\n",
       " 'provide': 140,\n",
       " 'notification': 129,\n",
       " 'turn': 162,\n",
       " 'numbers': 132,\n",
       " 'record': 141,\n",
       " 'are': 55,\n",
       " '281': 18,\n",
       " '584': 33,\n",
       " '3359': 23,\n",
       " 'voice': 166,\n",
       " '713': 39,\n",
       " '312': 20,\n",
       " '1689': 10,\n",
       " 'fax': 93,\n",
       " 'would': 171,\n",
       " 'you': 172,\n",
       " 'see': 146,\n",
       " 'someone': 150,\n",
       " 'contacts': 75,\n",
       " 'advises': 50,\n",
       " 'how': 114,\n",
       " 'submit': 153,\n",
       " 'future': 100,\n",
       " 'nominations': 127,\n",
       " 'via': 165,\n",
       " 'mail': 121,\n",
       " 'or': 135,\n",
       " 'thanks': 155,\n",
       " '09': 2,\n",
       " '44': 29,\n",
       " '43': 27,\n",
       " 'hi': 110,\n",
       " 'effective': 86,\n",
       " '11': 5,\n",
       " 'mscf': 124,\n",
       " 'min': 123,\n",
       " 'ftp': 99,\n",
       " 'time': 158,\n",
       " '925': 46,\n",
       " '24': 15,\n",
       " 'hours': 113,\n",
       " '908': 45,\n",
       " '878': 44,\n",
       " '840': 42,\n",
       " '793': 41,\n",
       " '14': 8,\n",
       " '738': 40,\n",
       " '674': 35,\n",
       " '18': 11,\n",
       " '602': 34,\n",
       " '20': 12,\n",
       " '521': 31,\n",
       " '22': 13,\n",
       " '431': 28,\n",
       " '332': 22,\n",
       " '26': 16,\n",
       " '224': 14,\n",
       " '28': 17,\n",
       " '108': 4,\n",
       " '30': 19,\n",
       " '982': 47,\n",
       " '32': 21,\n",
       " '847': 43,\n",
       " '703': 38,\n",
       " '36': 25,\n",
       " '549': 32,\n",
       " 'calpine': 68,\n",
       " 'daily': 79,\n",
       " 'doc': 83}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_transformer.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(ham_emails + spam_emails)\n",
    "y = np.array([0] * len(ham_emails) + [1] * len(spam_emails))\n",
    "\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (1, 224)\t2\n",
      "  (1, 312)\t3\n",
      "  (1, 249)\t2\n",
      "  (1, 275)\t2\n",
      "  (1, 277)\t2\n",
      "  (1, 146)\t2\n",
      "  (1, 169)\t1\n",
      "  (1, 326)\t1\n",
      "  (1, 0)\t2\n",
      "  (1, 91)\t4\n",
      "  (1, 240)\t3\n",
      "  (1, 157)\t3\n",
      "  (1, 204)\t2\n",
      "  (1, 25)\t11\n",
      "  (1, 3)\t18\n",
      "  (1, 349)\t4\n",
      "  (1, 356)\t5\n",
      "  (1, 305)\t1\n",
      "  (1, 106)\t1\n",
      "  (1, 230)\t7\n",
      "  (1, 108)\t2\n",
      "  (1, 217)\t3\n",
      "  (1, 365)\t5\n",
      "  (1, 251)\t10\n",
      "  (1, 188)\t17\n",
      "  :\t:\n",
      "  (1, 265)\t1\n",
      "  (1, 68)\t1\n",
      "  (1, 19)\t1\n",
      "  (1, 129)\t1\n",
      "  (1, 266)\t1\n",
      "  (1, 72)\t1\n",
      "  (1, 16)\t1\n",
      "  (1, 121)\t1\n",
      "  (1, 267)\t1\n",
      "  (1, 78)\t1\n",
      "  (1, 12)\t1\n",
      "  (1, 111)\t1\n",
      "  (1, 268)\t1\n",
      "  (1, 81)\t1\n",
      "  (1, 9)\t1\n",
      "  (1, 99)\t1\n",
      "  (2, 292)\t1\n",
      "  (2, 227)\t1\n",
      "  (2, 155)\t1\n",
      "  (2, 179)\t1\n",
      "  (2, 187)\t1\n",
      "  (2, 156)\t1\n",
      "  (2, 180)\t1\n",
      "  (2, 229)\t1\n",
      "  (2, 293)\t1\n",
      "['00', '00 500', '00 daren', '000', '000 108', '000 224', '000 332', '000 431', '000 521', '000 549', '000 602', '000 674', '000 703', '000 738', '000 793', '000 840', '000 847', '000 878', '000 908', '000 982', '000 gross', '000 increase', '09', '09 38', '09 44', '10', '10 00', '10 000', '10 16', '10 34', '10 38', '10 99', '108', '108 24', '11', '11 99', '12', '12 000', '12 10', '12 11', '12 13', '13', '13 99', '14', '14 000', '16', '16 000', '16 daren', '1689', '1689 fax', '18', '18 000', '20', '20 000', '22', '22 000', '224', '224 24', '24', '24 000', '24 hours', '26', '26 000', '28', '28 000', '281', '281 584', '30', '30 000', '312', '312 1689', '32', '32 000', '332', '332 24', '3359', '3359 voice', '34', '34 000', '34 enron', '36', '36 000', '38', '38 43', '38 carlos', '43', '43 george', '431', '431 24', '44', '44 linda', '500', '500 10', '500 925', '500 coming', '500 gross', '521', '521 24', '549', '549 24', '584', '584 3359', '602', '602 24', '674', '674 24', '68', '68 gross', '6992', '6992 forwarded', '703', '703 24', '713', '713 312', '738', '738 24', '793', '793 24', '840', '840 24', '847', '847 24', '878', '878 24', '908', '908 24', '925', '925 24', '982', '982 24', '99', '99 09', '99 10', '99 mscf', 'advises', 'advises submit', 'america', 'america corp', 'appears', 'appears nomination', 'attached', 'attached appears', 'based', 'based conversations', 'bill', 'bill fischer', 'block', 'block commenced', 'block previously', 'bmar', 'bmar forwarded', 'bryan', 'bryan hou', 'call', 'call linda', 'calpine', 'calpine daily', 'carlos', 'carlos expects', 'carlos please', 'carlos rodriguez', 'cc', 'cc gary', 'cc george', 'cc subject', 'coming', 'coming tomorrow', 'commence', 'commence production', 'commenced', 'commenced saturday', 'contacts', 'contacts linda', 'control', 'control provide', 'conversations', 'conversations bill', 'corp', 'corp george', 'daily', 'daily gas', 'daren', 'daren farmer', 'darren', 'darren attached', 'day', 'day based', 'doc', 'ect', 'ect 12', 'ect cc', 'ect ect', 'ect melissa', 'ect subject', 'effective', 'effective 12', 'enron', 'enron north', 'erroneously', 'erroneously referred', 'estimate', 'estimate 500', 'everything', 'everything set', 'expects', 'expects 500', 'expects well', 'farmer', 'farmer 12', 'farmer hou', 'fax', 'fax voice', 'fax would', 'fischer', 'fischer bmar', 'following', 'following day', 'forwarded', 'forwarded daren', 'forwarded george', 'ftp', 'ftp time', 'future', 'future nominations', 'gary', 'gary bryan', 'gary production', 'gas', 'gas control', 'gas nomination', 'george', 'george 6992', 'george weissman', 'get', 'get everything', 'get telephone', 'going', 'going estimate', 'graves', 'graves hou', 'gross', 'gross carlos', 'gross production', 'gross tomorrow', 'harris', 'harris 12', 'harris get', 'hi', 'hi effective', 'high', 'high island', 'hou', 'hou ect', 'hours', 'hours 000', 'hours 10', 'hours 12', 'hours 14', 'hours 16', 'hours 18', 'hours 20', 'hours 22', 'hours 24', 'hours 26', 'hours 28', 'hours 30', 'hours 32', 'hours 34', 'hours 36', 'inc', 'inc carlos', 'inc darren', 'inc high', 'increase', 'increase following', 'island', 'island larger', 'larger', 'larger block', 'linda', 'linda advises', 'linda get', 'linda harris', 'linda numbers', 'mail', 'mail fax', 'melissa', 'melissa graves', 'min', 'min ftp', 'mscf', 'mscf min', 'nomination', 'nomination doc', 'nomination vastar', 'nominations', 'nominations via', 'north', 'north america', 'notification', 'notification turn', 'number', 'number gas', 'numbers', 'numbers record', 'owns', 'owns 68', 'please', 'please call', 'please see', 'previously', 'previously erroneously', 'production', 'production george', 'production high', 'production sometime', 'provide', 'provide notification', 'record', 'record 281', 'referred', 'referred well', 'resources', 'resources inc', 'rodriguez', 'rodriguez hou', 'saturday', 'saturday 00', 'see', 'see someone', 'set', 'set going', 'someone', 'someone contacts', 'sometime', 'sometime tomorrow', 'subject', 'subject hi', 'subject vastar', 'submit', 'submit future', 'telephone', 'telephone number', 'thanks', 'thanks george', 'time', 'time 500', 'told', 'told linda', 'tomorrow', 'tomorrow 000', 'tomorrow linda', 'tomorrow told', 'tomorrow vastar', 'turn', 'turn tomorrow', 'vastar', 'vastar expects', 'vastar owns', 'vastar resources', 'via', 'via mail', 'voice', 'voice 713', 'voice thanks', 'weissman', 'weissman 12', 'weissman hou', 'well', 'well commence', 'well vastar', 'would', 'would please']\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(stop_words=stopwords.words('english'), ngram_range=(1, 2))\n",
    "print(vectorizer.fit_transform(X_few_text))\n",
    "print(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "email_pipeline = Pipeline([\n",
    "    (\"EmailJson to Words\", EmailJsonToText()),\n",
    "    (\"Words to Count Vector\", CountVectorizer(stop_words=stopwords.words('english'), ngram_range=(1, 2))),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_augmented_train = email_pipeline.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9719603577471597"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_clf = LogisticRegression(solver=\"liblinear\", random_state=42)\n",
    "score = model_selection.cross_val_score(log_clf, X_augmented_train, y_train, cv=3)\n",
    "score.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 91.53%\n",
      "Recall: 98.25%\n"
     ]
    }
   ],
   "source": [
    "X_augmented_test = email_pipeline.transform(X_test)\n",
    "\n",
    "log_clf = LogisticRegression(solver=\"liblinear\", random_state=42)\n",
    "log_clf.fit(X_augmented_train, y_train)\n",
    "\n",
    "y_pred = log_clf.predict(X_augmented_test)\n",
    "\n",
    "print(\"Precision: {:.2f}%\".format(100 * precision_score(y_test, y_pred)))\n",
    "print(\"Recall: {:.2f}%\".format(100 * recall_score(y_test, y_pred)))\n",
    "\n",
    "# only added stopwords, added ngrams -> precision 91.53%, recall 98.25%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "246977\n"
     ]
    }
   ],
   "source": [
    "print(len(email_pipeline.named_steps['Words to Count Vector'].get_feature_names()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 91.15%\n",
      "Recall: 97.20%\n"
     ]
    }
   ],
   "source": [
    "email_pipeline2 = Pipeline([\n",
    "    (\"EmailJson to Words\", EmailJsonToText(stemming=True, numberReplacement=True)),\n",
    "    (\"Words to Count Vector\", CountVectorizer(stop_words=stopwords.words('english'), ngram_range=(1, 2))),\n",
    "])\n",
    "\n",
    "X_augmented_train2 = email_pipeline2.fit_transform(X_train)\n",
    "X_augmented_test2 = email_pipeline2.transform(X_test)\n",
    "\n",
    "log_clf = LogisticRegression(solver=\"liblinear\", random_state=42)\n",
    "log_clf.fit(X_augmented_train2, y_train)\n",
    "\n",
    "y_pred2 = log_clf.predict(X_augmented_test2)\n",
    "\n",
    "print(\"Precision: {:.2f}%\".format(100 * precision_score(y_test, y_pred2)))\n",
    "print(\"Recall: {:.2f}%\".format(100 * recall_score(y_test, y_pred2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "216467\n"
     ]
    }
   ],
   "source": [
    "print(len(email_pipeline2.named_steps['Words to Count Vector'].get_feature_names()))\n",
    "# with only stopword removal, with bi-gram support\n",
    "# 246977 features and 91.53% precision & 98.25% recall\n",
    "\n",
    "# with less non-sensical features, worse results??\n",
    "# with no numbers, without stemming\n",
    "# 224292 features and 91.78% precision & 97.55% recall\n",
    "\n",
    "# with no numbers, with stemming\n",
    "# 216467 features and 91.15% precision & 97.20% recall\n",
    "\n",
    "# with no bi-gram support, with no numbers, with stemming\n",
    "# 35171 features and 91.50% precision & 97.90% recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 373)\n",
      "  (1, 99)\t0.018790630518289823\n",
      "  (1, 9)\t0.018790630518289823\n",
      "  (1, 81)\t0.018790630518289823\n",
      "  (1, 268)\t0.018790630518289823\n",
      "  (1, 111)\t0.018790630518289823\n",
      "  (1, 12)\t0.018790630518289823\n",
      "  (1, 78)\t0.018790630518289823\n",
      "  (1, 267)\t0.018790630518289823\n",
      "  (1, 121)\t0.018790630518289823\n",
      "  (1, 16)\t0.018790630518289823\n",
      "  (1, 72)\t0.018790630518289823\n",
      "  (1, 266)\t0.018790630518289823\n",
      "  (1, 129)\t0.018790630518289823\n",
      "  (1, 19)\t0.018790630518289823\n",
      "  (1, 68)\t0.018790630518289823\n",
      "  (1, 265)\t0.018790630518289823\n",
      "  (1, 33)\t0.018790630518289823\n",
      "  (1, 4)\t0.018790630518289823\n",
      "  (1, 64)\t0.018790630518289823\n",
      "  (1, 264)\t0.018790630518289823\n",
      "  (1, 57)\t0.018790630518289823\n",
      "  (1, 5)\t0.018790630518289823\n",
      "  (1, 62)\t0.018790630518289823\n",
      "  (1, 263)\t0.018790630518289823\n",
      "  (1, 74)\t0.018790630518289823\n",
      "  :\t:\n",
      "  (1, 349)\t0.0751625220731593\n",
      "  (1, 3)\t0.3382313493292168\n",
      "  (1, 25)\t0.20669693570118802\n",
      "  (1, 204)\t0.03758126103657965\n",
      "  (1, 157)\t0.05637189155486947\n",
      "  (1, 240)\t0.05637189155486947\n",
      "  (1, 91)\t0.0751625220731593\n",
      "  (1, 0)\t0.03758126103657965\n",
      "  (1, 326)\t0.018790630518289823\n",
      "  (1, 169)\t0.018790630518289823\n",
      "  (1, 146)\t0.03758126103657965\n",
      "  (1, 277)\t0.03758126103657965\n",
      "  (1, 275)\t0.03758126103657965\n",
      "  (1, 249)\t0.03758126103657965\n",
      "  (1, 312)\t0.05637189155486947\n",
      "  (1, 224)\t0.03758126103657965\n",
      "  (2, 293)\t0.3501387057719138\n",
      "  (2, 229)\t0.3501387057719138\n",
      "  (2, 180)\t0.3501387057719138\n",
      "  (2, 156)\t0.3501387057719138\n",
      "  (2, 187)\t0.3501387057719138\n",
      "  (2, 179)\t0.3501387057719138\n",
      "  (2, 155)\t0.3501387057719138\n",
      "  (2, 227)\t0.2662895107233706\n",
      "  (2, 292)\t0.2662895107233706\n",
      "[[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.03758126 0.01879063 0.01879063 ... 0.01879063 0.01879063 0.01879063]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]]\n",
      "['00', '00 500', '00 daren', '000', '000 108', '000 224', '000 332', '000 431', '000 521', '000 549', '000 602', '000 674', '000 703', '000 738', '000 793', '000 840', '000 847', '000 878', '000 908', '000 982', '000 gross', '000 increase', '09', '09 38', '09 44', '10', '10 00', '10 000', '10 16', '10 34', '10 38', '10 99', '108', '108 24', '11', '11 99', '12', '12 000', '12 10', '12 11', '12 13', '13', '13 99', '14', '14 000', '16', '16 000', '16 daren', '1689', '1689 fax', '18', '18 000', '20', '20 000', '22', '22 000', '224', '224 24', '24', '24 000', '24 hours', '26', '26 000', '28', '28 000', '281', '281 584', '30', '30 000', '312', '312 1689', '32', '32 000', '332', '332 24', '3359', '3359 voice', '34', '34 000', '34 enron', '36', '36 000', '38', '38 43', '38 carlos', '43', '43 george', '431', '431 24', '44', '44 linda', '500', '500 10', '500 925', '500 coming', '500 gross', '521', '521 24', '549', '549 24', '584', '584 3359', '602', '602 24', '674', '674 24', '68', '68 gross', '6992', '6992 forwarded', '703', '703 24', '713', '713 312', '738', '738 24', '793', '793 24', '840', '840 24', '847', '847 24', '878', '878 24', '908', '908 24', '925', '925 24', '982', '982 24', '99', '99 09', '99 10', '99 mscf', 'advises', 'advises submit', 'america', 'america corp', 'appears', 'appears nomination', 'attached', 'attached appears', 'based', 'based conversations', 'bill', 'bill fischer', 'block', 'block commenced', 'block previously', 'bmar', 'bmar forwarded', 'bryan', 'bryan hou', 'call', 'call linda', 'calpine', 'calpine daily', 'carlos', 'carlos expects', 'carlos please', 'carlos rodriguez', 'cc', 'cc gary', 'cc george', 'cc subject', 'coming', 'coming tomorrow', 'commence', 'commence production', 'commenced', 'commenced saturday', 'contacts', 'contacts linda', 'control', 'control provide', 'conversations', 'conversations bill', 'corp', 'corp george', 'daily', 'daily gas', 'daren', 'daren farmer', 'darren', 'darren attached', 'day', 'day based', 'doc', 'ect', 'ect 12', 'ect cc', 'ect ect', 'ect melissa', 'ect subject', 'effective', 'effective 12', 'enron', 'enron north', 'erroneously', 'erroneously referred', 'estimate', 'estimate 500', 'everything', 'everything set', 'expects', 'expects 500', 'expects well', 'farmer', 'farmer 12', 'farmer hou', 'fax', 'fax voice', 'fax would', 'fischer', 'fischer bmar', 'following', 'following day', 'forwarded', 'forwarded daren', 'forwarded george', 'ftp', 'ftp time', 'future', 'future nominations', 'gary', 'gary bryan', 'gary production', 'gas', 'gas control', 'gas nomination', 'george', 'george 6992', 'george weissman', 'get', 'get everything', 'get telephone', 'going', 'going estimate', 'graves', 'graves hou', 'gross', 'gross carlos', 'gross production', 'gross tomorrow', 'harris', 'harris 12', 'harris get', 'hi', 'hi effective', 'high', 'high island', 'hou', 'hou ect', 'hours', 'hours 000', 'hours 10', 'hours 12', 'hours 14', 'hours 16', 'hours 18', 'hours 20', 'hours 22', 'hours 24', 'hours 26', 'hours 28', 'hours 30', 'hours 32', 'hours 34', 'hours 36', 'inc', 'inc carlos', 'inc darren', 'inc high', 'increase', 'increase following', 'island', 'island larger', 'larger', 'larger block', 'linda', 'linda advises', 'linda get', 'linda harris', 'linda numbers', 'mail', 'mail fax', 'melissa', 'melissa graves', 'min', 'min ftp', 'mscf', 'mscf min', 'nomination', 'nomination doc', 'nomination vastar', 'nominations', 'nominations via', 'north', 'north america', 'notification', 'notification turn', 'number', 'number gas', 'numbers', 'numbers record', 'owns', 'owns 68', 'please', 'please call', 'please see', 'previously', 'previously erroneously', 'production', 'production george', 'production high', 'production sometime', 'provide', 'provide notification', 'record', 'record 281', 'referred', 'referred well', 'resources', 'resources inc', 'rodriguez', 'rodriguez hou', 'saturday', 'saturday 00', 'see', 'see someone', 'set', 'set going', 'someone', 'someone contacts', 'sometime', 'sometime tomorrow', 'subject', 'subject hi', 'subject vastar', 'submit', 'submit future', 'telephone', 'telephone number', 'thanks', 'thanks george', 'time', 'time 500', 'told', 'told linda', 'tomorrow', 'tomorrow 000', 'tomorrow linda', 'tomorrow told', 'tomorrow vastar', 'turn', 'turn tomorrow', 'vastar', 'vastar expects', 'vastar owns', 'vastar resources', 'via', 'via mail', 'voice', 'voice 713', 'voice thanks', 'weissman', 'weissman 12', 'weissman hou', 'well', 'well commence', 'well vastar', 'would', 'would please']\n"
     ]
    }
   ],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(stop_words=stopwords.words('english'), ngram_range=(1, 2))\n",
    "tfidf_res = tfidf_vectorizer.fit_transform(X_few_text)\n",
    "print(tfidf_res.shape)\n",
    "print(tfidf_res)\n",
    "# instead of counts, we have tfidf values for each word, based on the corpus (X_few_text in this case)\n",
    "print(tfidf_res.todense())\n",
    "print(tfidf_vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 90.78%\n",
      "Recall: 93.01%\n"
     ]
    }
   ],
   "source": [
    "email_pipeline3 = Pipeline([\n",
    "    (\"EmailJson to Words\", EmailJsonToText(stemming=True, numberReplacement=True)),\n",
    "    (\"Words to TF-IDF Vector\", TfidfVectorizer(stop_words=stopwords.words('english'), ngram_range=(1, 2))),\n",
    "])\n",
    "\n",
    "X_augmented_train3 = email_pipeline3.fit_transform(X_train)\n",
    "X_augmented_test3 = email_pipeline3.transform(X_test)\n",
    "\n",
    "xgb_clf = XGBClassifier()\n",
    "xgb_clf.fit(X_augmented_train3, y_train)\n",
    "\n",
    "y_pred3 = xgb_clf.predict(X_augmented_test3)\n",
    "\n",
    "print(\"Precision: {:.2f}%\".format(100 * precision_score(y_test, y_pred3)))\n",
    "print(\"Recall: {:.2f}%\".format(100 * recall_score(y_test, y_pred3)))\n",
    "\n",
    "# https://xgboost.readthedocs.io/en/latest/python/python_api.html#xgboost.XGBClassifier\n",
    "xgb_clf.save_model('enron1_trained_xgb_model.bin')\n",
    "\n",
    "# generic xgboost guide\n",
    "# https://github.com/dmlc/xgboost/tree/master/demo/guide-python\n",
    "\n",
    "# interestingly logistic regression outperforms xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 1 0 0 0 0 1 0 0]\n",
      "{'subject': 'Subject: conoco , inc . katy tailgate contract 96001985 sitara 334995\\n', 'body': ['bob , the referenced contract dated 1 / 1 / 96 as amended effective 4 / 1 / 2000 does include an evergreen provision . thus , please add may , 2001 to the existing sitara deal , 334995 , at the current price , 90 % of ifhsc , for a volume of 155 mmbtu / d .\\n', 'don , please prepare and circulate a termination letter for 96001985 .\\n', 'george x 3 - 6992\\n', '- - - - - - - - - - - - - - - - - - - - - - forwarded by george weissman / hou / ect on 04 / 30 / 2001 02 : 11 pm - - - - - - - - - - - - - - - - - - - - - - - - - - -\\n', 'john peyton\\n', '04 / 30 / 2001 01 : 08 pm\\n', 'to : george weissman / hou / ect @ ect\\n', 'cc :\\n', 'subject : conoco katy contract\\n', \"please send conoco ' s mike luchetti two month - to - month contracts for april and may volumes of around 150 - 200 mmbtu / day . this is for facility number 10077 - exxon plant hpl katy for gas that was being purchased under a contract that expired april 1 , 2001 - contract no 012 - 19785 - 113 .\\n\", 'thank you ,\\n', 'john']}\n"
     ]
    }
   ],
   "source": [
    "print(y_train[0:10])\n",
    "print(X_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4137,)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_df = pd.Series(EmailJsonToText(stemming=True, numberReplacement=True).fit_transform(X_train))\n",
    "# X_df = pd.Series(X_train)\n",
    "y_df = pd.Series(y_train)\n",
    "X_df.name = 'text'\n",
    "X_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bob  the referenc contract date %d / %d / %d%d as amend effect %d / %d / %d%d%d%d doe includ an evergreen provis  thus  pleas add may  %d%d%d%d to the exist sitara deal  %d%d%d%d%d%d  at the current price  %d%d % of ifhsc  for a volum of %d%d%d mmbtu / d \n",
      " don  pleas prepar and circul a termin letter for %d%d%d%d%d%d%d%d \n",
      " georg x %d - %d%d%d%d\n",
      " - - - - - - - - - - - - - - - - - - - - - - forward by georg weissman / hou / ect on %d%d / %d%d / %d%d%d%d %d%d : %d%d pm - - - - - - - - - - - - - - - - - - - - - - - - - - -\n",
      " john peyton\n",
      " %d%d / %d%d / %d%d%d%d %d%d : %d%d pm\n",
      " to : georg weissman / hou / ect @ ect\n",
      " cc :\n",
      " subject : conoco kati contract\n",
      " pleas send conoco ' s mike luchetti two month - to - month contract for april and may volum of around %d%d%d - %d%d%d mmbtu / day  this is for facil number %d%d%d%d%d - exxon plant hpl kati for gas that was be purchas under a contract that expir april %d  %d%d%d%d - contract no %d%d%d - %d%d%d%d%d - %d%d%d \n",
      " thank you \n",
      " john\n",
      "1\n",
      "<class 'pandas.core.series.Series'>\n",
      "(10,)\n",
      "bob  the referenc contract date %d / %d / %d%d as amend effect %d / %d / %d%d%d%d doe includ an evergreen provis  thus  pleas add may  %d%d%d%d to the exist sitara deal  %d%d%d%d%d%d  at the current price  %d%d % of ifhsc  for a volum of %d%d%d mmbtu / d \n",
      " don  pleas prepar and circul a termin letter for %d%d%d%d%d%d%d%d \n",
      " georg x %d - %d%d%d%d\n",
      " - - - - - - - - - - - - - - - - - - - - - - forward by georg weissman / hou / ect on %d%d / %d%d / %d%d%d%d %d%d : %d%d pm - - - - - - - - - - - - - - - - - - - - - - - - - - -\n",
      " john peyton\n",
      " %d%d / %d%d / %d%d%d%d %d%d : %d%d pm\n",
      " to : georg weissman / hou / ect @ ect\n",
      " cc :\n",
      " subject : conoco kati contract\n",
      " pleas send conoco ' s mike luchetti two month - to - month contract for april and may volum of around %d%d%d - %d%d%d mmbtu / day  this is for facil number %d%d%d%d%d - exxon plant hpl kati for gas that was be purchas under a contract that expir april %d  %d%d%d%d - contract no %d%d%d - %d%d%d%d%d - %d%d%d \n",
      " thank you \n",
      " john\n",
      "fyi\n",
      " from : robert cotten @ ect %d%d / %d%d / %d%d%d%d %d%d : %d%d am\n",
      " to : vanc l taylor / hou / ect @ ect\n",
      " cc : pat clyne / corp / enron @ enron  o ' neal d winfre / hou / ect @ ect\n",
      " subject : meter # %d%d%d%d - lundel ranch c / p - gato creek\n",
      " vanc \n",
      " it appear the actual volum have been signific higher than nominations\n",
      " at the subject meter the past sever month  the follow represents\n",
      " activ dure the month of june through septemb :\n",
      " gas month total nom mmbtu total actual mmbtu\n",
      " %d%d / %d%d%d%d %d%d  %d%d%d %d%d%d  %d%d%d\n",
      " %d%d / %d%d%d%d %d%d  %d%d%d %d%d%d  %d%d%d\n",
      " %d%d / %d%d%d%d %d%d  %d%d%d %d%d%d  %d%d%d\n",
      " %d%d / %d%d%d%d %d%d  %d%d%d %d%d%d  %d%d%d\n",
      " deal # %d%d%d%d%d%d  calpin natur gas compani  is the onli activ at this\n",
      " meter  should we adjust the nomin to more close resembl the actual\n",
      " volum  pleas advis  thank \n",
      " bob\n",
      "hi \n",
      " find lot of cheap softwar in our site \n",
      " the popular programs\n",
      " adob photoshop cs %d  %d for onli %d%d $ \n",
      " dvdxcopi platinum %d  %d  %d%d for %d%d $ \n",
      " microsoft offic system profession %d%d%d%d ( %d cds ) for %d%d $ \n",
      " corel draw %d%d graphic suit ( %d cds ) for just %d%d $ \n",
      " roxio easi media creator %d  %d for %d%d $ \n",
      " and much  much more    \n",
      " take a look on the complet list \n",
      " wonder whi our priic are unbeliev low \n",
      " we are current clear our good at incredibili cheeap sale - price in connect with the shutdown of our shop and the closur of the stockhous  don ' t missss your lucki chanc to get the best pricc on dis - count softwar \n",
      " regards\n",
      " sharron whitten\n",
      " no thank :\n",
      " - - - - - - - - - - - - - - - - - - - - - - - -\n",
      " sullivan complain bestir absolv adiabat cosmolog mysteri disrupt enabl thiev appar traips but septic rush wrongdo scotch wearisom arisen channel lithospher crossway eduardo baron compliment purgatori darken ultra belief chaplainci allison\n",
      " limpid mayappl game garbag buddha cortex abstin sensori chimer nakayama effloresc lima abod initi curfew cadent\n",
      " chevron lucill melvin archangel broccoli doll dod germanium acquir stepwis conven dobbin abut convict buten positron mcgill bullhid beaver editori dispers abus taoist expel sherwood splash gauss crumb reclam hobb war berglund trespass inter scotland martinez midwiv flame bertram cryogen finley allegor eli fetter denni asthma alien confess\n",
      " lipton beacon\n",
      " headach albanian juxtaposit permian janice\n",
      " dimethyl conservatori crossroad civic bloodstream bryozoa odin bureaucrat mccarthi skindiv silic go drib capistrano snap clonic duel chaplainci ethan\n",
      " bloomfield shi anaglyph revelri tortoiseshel cayuga still telegraphi flagler prow elijah interstic assassin ballroom tuscarora\n",
      " satiat pvc bronchiol thicket bess nobodi folk capybara menarch plant particular deaco margarin ornament larva cilia result pocono basketbal drastic hypotenus sailor nv officio lightfac siva mensur spheric germanium purslan sightseer newbold methuen polymer essenti blueback moribund vitiat blustery\n",
      " apport aquarius debat astound deton provinci down tactil sidearm credit filmdom codfish persimmon hypoact chimney getaway oilman indivis claremont maier ghent diorama manserv edith rebel vagari nashvill emolu jitterbug atop chronicl balboa analog thirteen opec fail vitreous palac schoenberg stepson libya almost aerodynam cotyledon doubt mile ptolemi sight blown billy\n",
      " abet molybdenit pain hyanni fleawort theti illog consist veranda condol configur beli copolym bloodlin masoch bruit ok dilut pharaoh carboxi alvin drumlin protrud septenni europa facet lore blown draftsman anod dusenburi skillful\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(X_df.iloc[0])\n",
    "print(y_df.iloc[2])\n",
    "print(type(X_df[:10]))\n",
    "print(X_df[:10].shape)\n",
    "for X in X_df[:3]: \n",
    "    print(X)\n",
    "\n",
    "# if X_df was a DataFrame instead of Series\n",
    "# for index, row in X_df[:10].iterrows():\n",
    "#     print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text\n",
      "text\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4137"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(X_df.name)\n",
    "print((np.asarray(X_df.name)).astype(str))\n",
    "\n",
    "len(X_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0                                                     \n",
      "1    gary  production from the high island larger b...\n",
      "2                - calpine daily gas nomination 1  doc\n",
      "dtype: object\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<3x373 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 375 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(pd.Series(X_few_text))\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words=stopwords.words('english'), ngram_range=(1, 2))\n",
    "tfidf_res = tfidf_vectorizer.fit_transform(pd.Series(X_few_text))\n",
    "tfidf_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/michaelchen/.local/share/virtualenvs/hands-on-with-ml-xI9Nv3kk/lib/python3.8/site-packages/sklearn/base.py:209: FutureWarning: From version 0.24, get_params will raise an AttributeError if a parameter cannot be retrieved as an instance attribute. Previously it would return None.\n",
      "  warnings.warn('From version 0.24, get_params will raise an '\n",
      "/usr/local/opt/python@3.8/Frameworks/Python.framework/Versions/3.8/lib/python3.8/subprocess.py:844: RuntimeWarning: line buffering (buffering=1) isn't supported in binary mode, the default buffer size will be used\n",
      "  self.stdout = io.open(c2pread, 'rb', bufsize)\n",
      "/usr/local/opt/python@3.8/Frameworks/Python.framework/Versions/3.8/lib/python3.8/subprocess.py:849: RuntimeWarning: line buffering (buffering=1) isn't supported in binary mode, the default buffer size will be used\n",
      "  self.stderr = io.open(errread, 'rb', bufsize)\n"
     ]
    }
   ],
   "source": [
    "from sklearn2pmml import sklearn2pmml, make_pmml_pipeline\n",
    "from sklearn2pmml.pipeline import PMMLPipeline\n",
    "from sklearn2pmml.feature_extraction.text import Splitter\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "\n",
    "selector = SelectKBest(k=5000)\n",
    "\n",
    "pipeline = PMMLPipeline([\n",
    "#     (\"EmailJson to Words\", EmailJsonToText(stemming=True, numberReplacement=True)),\n",
    "    (\"Words to TF-IDF Vector\", TfidfVectorizer(\n",
    "        stop_words=stopwords.words('english'),\n",
    "        ngram_range=(1, 2),\n",
    "        tokenizer=Splitter(),\n",
    "        norm=None)),\n",
    "    (\"Selector\", selector),\n",
    "    (\"Logistic Reg\", LogisticRegression(solver=\"liblinear\", multi_class=\"ovr\", random_state=42))\n",
    "])\n",
    "# pipeline.fit(X_train, y_train)\n",
    "pipeline.fit(X_df, y_df)\n",
    "\n",
    "# pipeline = make_pmml_pipeline(\n",
    "#     pipeline,\n",
    "#     active_fields = ['text'],\n",
    "# )\n",
    "# pipeline.verify(X_train[0:10])\n",
    "pipeline.verify(X_df[:10])\n",
    "\n",
    "sklearn2pmml(pipeline, \"LogRegPipeline.pmml\", with_repr = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5,\n",
       "             estimator=Pipeline(steps=[('email_json_to_words',\n",
       "                                        EmailJsonToText()),\n",
       "                                       ('word_vect',\n",
       "                                        CountVectorizer(stop_words=['i', 'me',\n",
       "                                                                    'my',\n",
       "                                                                    'myself',\n",
       "                                                                    'we', 'our',\n",
       "                                                                    'ours',\n",
       "                                                                    'ourselves',\n",
       "                                                                    'you',\n",
       "                                                                    \"you're\",\n",
       "                                                                    \"you've\",\n",
       "                                                                    \"you'll\",\n",
       "                                                                    \"you'd\",\n",
       "                                                                    'your',\n",
       "                                                                    'yours',\n",
       "                                                                    'yourself',\n",
       "                                                                    'yourselves',\n",
       "                                                                    'he', 'him',\n",
       "                                                                    'his',\n",
       "                                                                    'himself',\n",
       "                                                                    'she',\n",
       "                                                                    \"she's\",\n",
       "                                                                    'her',\n",
       "                                                                    'hers',\n",
       "                                                                    'herself',\n",
       "                                                                    'it',\n",
       "                                                                    \"it's\",\n",
       "                                                                    'its',\n",
       "                                                                    'itself', ...])),\n",
       "                                       ('tfidf', TfidfTransformer()),\n",
       "                                       ('logistic_reg',\n",
       "                                        LogisticRegression(random_state=42))]),\n",
       "             n_jobs=4,\n",
       "             param_grid={'email_json_to_words__numberReplacement': (True,\n",
       "                                                                    False),\n",
       "                         'email_json_to_words__stemming': [True, False],\n",
       "                         'logistic_reg__solver': ['liblinear', 'lbfgs'],\n",
       "                         'tfidf__use_idf': (True, False),\n",
       "                         'word_vect__ngram_range': [(1, 1), (1, 2)]})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gridsearch_email_pipeline = Pipeline([\n",
    "    (\"email_json_to_words\", EmailJsonToText()),\n",
    "    (\"word_vect\", CountVectorizer(stop_words=stopwords.words('english'))),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('logistic_reg', LogisticRegression(random_state=42)),\n",
    "])\n",
    "\n",
    "parameters = {\n",
    "    'email_json_to_words__stemming': [True, False],\n",
    "    'email_json_to_words__numberReplacement': (True, False),\n",
    "    'word_vect__ngram_range': [(1, 1), (1, 2)],\n",
    "    'tfidf__use_idf': (True, False),\n",
    "    'logistic_reg__solver': ['liblinear', 'lbfgs'],\n",
    "}\n",
    "\n",
    "gs_clf = model_selection.GridSearchCV(gridsearch_email_pipeline, parameters, cv=5, n_jobs=4)\n",
    "gs_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([1.25238079e+01, 1.46737866e+01, 1.36392612e+01, 1.34990186e+01,\n",
       "        1.27570862e+01, 1.61789546e+01, 1.35043321e+01, 1.48130617e+01,\n",
       "        3.56695910e+00, 4.76409750e+00, 4.02849107e+00, 5.54905529e+00,\n",
       "        4.18416357e+00, 5.72632990e+00, 4.08213882e+00, 5.70046964e+00,\n",
       "        1.16070148e+01, 2.54774218e+01, 1.89019210e+01, 1.45211731e+03,\n",
       "        2.16980710e+03, 1.97213035e+03, 4.14298757e+03, 1.51739255e+04,\n",
       "        3.88312813e+02, 3.71516781e+00, 2.73716388e+00, 3.71986012e+00,\n",
       "        3.04679751e+00, 5.19847121e+00, 3.36701183e+00, 5.20884643e+00]),\n",
       " 'std_fit_time': array([5.50209358e-01, 3.85515889e-01, 3.75966177e-01, 4.19094250e-01,\n",
       "        1.18815987e+00, 6.66094943e-01, 4.19564581e-01, 6.57750470e-01,\n",
       "        2.85355023e-01, 2.68869407e-01, 1.55597733e-01, 1.90529509e-01,\n",
       "        1.78468120e-01, 3.31276981e-01, 1.80692661e-01, 1.59260243e-01,\n",
       "        4.02278143e-01, 1.61805891e+01, 1.28907177e+01, 2.88000906e+03,\n",
       "        2.99234199e+03, 6.80667937e+02, 3.80113417e+03, 6.90519719e+03,\n",
       "        7.71461380e+02, 1.79566373e-01, 1.57540115e-01, 1.05845745e-01,\n",
       "        2.19195365e-01, 2.45244952e-01, 8.51423663e-02, 2.80468788e-01]),\n",
       " 'mean_score_time': array([3.31516356e+00, 3.42577162e+00, 3.13664212e+00, 3.51659961e+00,\n",
       "        3.15420122e+00, 3.58576016e+00, 3.74106069e+00, 3.73526945e+00,\n",
       "        7.94188213e-01, 9.57146931e-01, 8.73389292e-01, 9.63495159e-01,\n",
       "        8.37063742e-01, 9.68809891e-01, 8.06856394e-01, 8.69961119e-01,\n",
       "        2.71588454e+00, 7.90792799e+00, 2.66565366e+00, 3.06164289e+03,\n",
       "        2.28477650e+00, 7.19748314e+02, 2.16137661e+03, 3.24120627e+03,\n",
       "        6.45237255e-01, 7.35756731e-01, 6.08198166e-01, 7.09154415e-01,\n",
       "        6.74226761e-01, 8.02932596e-01, 6.87753153e-01, 7.25895929e-01]),\n",
       " 'std_score_time': array([1.79436989e-01, 2.08439792e-01, 1.67874489e-01, 2.08065594e-01,\n",
       "        2.64692241e-01, 1.61549298e-01, 4.37885220e-01, 3.61957541e-01,\n",
       "        7.66317648e-02, 1.67148957e-01, 7.22790202e-02, 8.40445893e-02,\n",
       "        1.09874467e-01, 1.31835051e-01, 9.67680545e-02, 6.67316964e-02,\n",
       "        1.18107441e-01, 1.04163106e+01, 1.61050889e-01, 3.39621442e+03,\n",
       "        1.39364348e-01, 3.58725140e+02, 4.31751777e+03, 3.84488962e+03,\n",
       "        1.21293094e-01, 1.30373973e-01, 5.56183860e-02, 7.71649009e-02,\n",
       "        8.97172461e-02, 9.32810777e-02, 5.32271824e-02, 1.04753949e-01]),\n",
       " 'param_email_json_to_words__numberReplacement': masked_array(data=[True, True, True, True, True, True, True, True, True,\n",
       "                    True, True, True, True, True, True, True, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_email_json_to_words__stemming': masked_array(data=[True, True, True, True, True, True, True, True, False,\n",
       "                    False, False, False, False, False, False, False, True,\n",
       "                    True, True, True, True, True, True, True, False, False,\n",
       "                    False, False, False, False, False, False],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_logistic_reg__solver': masked_array(data=['liblinear', 'liblinear', 'liblinear', 'liblinear',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'liblinear',\n",
       "                    'liblinear', 'liblinear', 'liblinear', 'lbfgs',\n",
       "                    'lbfgs', 'lbfgs', 'lbfgs', 'liblinear', 'liblinear',\n",
       "                    'liblinear', 'liblinear', 'lbfgs', 'lbfgs', 'lbfgs',\n",
       "                    'lbfgs', 'liblinear', 'liblinear', 'liblinear',\n",
       "                    'liblinear', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs'],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_tfidf__use_idf': masked_array(data=[True, True, False, False, True, True, False, False,\n",
       "                    True, True, False, False, True, True, False, False,\n",
       "                    True, True, False, False, True, True, False, False,\n",
       "                    True, True, False, False, True, True, False, False],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_word_vect__ngram_range': masked_array(data=[(1, 1), (1, 2), (1, 1), (1, 2), (1, 1), (1, 2), (1, 1),\n",
       "                    (1, 2), (1, 1), (1, 2), (1, 1), (1, 2), (1, 1), (1, 2),\n",
       "                    (1, 1), (1, 2), (1, 1), (1, 2), (1, 1), (1, 2), (1, 1),\n",
       "                    (1, 2), (1, 1), (1, 2), (1, 1), (1, 2), (1, 1), (1, 2),\n",
       "                    (1, 1), (1, 2), (1, 1), (1, 2)],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'params': [{'email_json_to_words__numberReplacement': True,\n",
       "   'email_json_to_words__stemming': True,\n",
       "   'logistic_reg__solver': 'liblinear',\n",
       "   'tfidf__use_idf': True,\n",
       "   'word_vect__ngram_range': (1, 1)},\n",
       "  {'email_json_to_words__numberReplacement': True,\n",
       "   'email_json_to_words__stemming': True,\n",
       "   'logistic_reg__solver': 'liblinear',\n",
       "   'tfidf__use_idf': True,\n",
       "   'word_vect__ngram_range': (1, 2)},\n",
       "  {'email_json_to_words__numberReplacement': True,\n",
       "   'email_json_to_words__stemming': True,\n",
       "   'logistic_reg__solver': 'liblinear',\n",
       "   'tfidf__use_idf': False,\n",
       "   'word_vect__ngram_range': (1, 1)},\n",
       "  {'email_json_to_words__numberReplacement': True,\n",
       "   'email_json_to_words__stemming': True,\n",
       "   'logistic_reg__solver': 'liblinear',\n",
       "   'tfidf__use_idf': False,\n",
       "   'word_vect__ngram_range': (1, 2)},\n",
       "  {'email_json_to_words__numberReplacement': True,\n",
       "   'email_json_to_words__stemming': True,\n",
       "   'logistic_reg__solver': 'lbfgs',\n",
       "   'tfidf__use_idf': True,\n",
       "   'word_vect__ngram_range': (1, 1)},\n",
       "  {'email_json_to_words__numberReplacement': True,\n",
       "   'email_json_to_words__stemming': True,\n",
       "   'logistic_reg__solver': 'lbfgs',\n",
       "   'tfidf__use_idf': True,\n",
       "   'word_vect__ngram_range': (1, 2)},\n",
       "  {'email_json_to_words__numberReplacement': True,\n",
       "   'email_json_to_words__stemming': True,\n",
       "   'logistic_reg__solver': 'lbfgs',\n",
       "   'tfidf__use_idf': False,\n",
       "   'word_vect__ngram_range': (1, 1)},\n",
       "  {'email_json_to_words__numberReplacement': True,\n",
       "   'email_json_to_words__stemming': True,\n",
       "   'logistic_reg__solver': 'lbfgs',\n",
       "   'tfidf__use_idf': False,\n",
       "   'word_vect__ngram_range': (1, 2)},\n",
       "  {'email_json_to_words__numberReplacement': True,\n",
       "   'email_json_to_words__stemming': False,\n",
       "   'logistic_reg__solver': 'liblinear',\n",
       "   'tfidf__use_idf': True,\n",
       "   'word_vect__ngram_range': (1, 1)},\n",
       "  {'email_json_to_words__numberReplacement': True,\n",
       "   'email_json_to_words__stemming': False,\n",
       "   'logistic_reg__solver': 'liblinear',\n",
       "   'tfidf__use_idf': True,\n",
       "   'word_vect__ngram_range': (1, 2)},\n",
       "  {'email_json_to_words__numberReplacement': True,\n",
       "   'email_json_to_words__stemming': False,\n",
       "   'logistic_reg__solver': 'liblinear',\n",
       "   'tfidf__use_idf': False,\n",
       "   'word_vect__ngram_range': (1, 1)},\n",
       "  {'email_json_to_words__numberReplacement': True,\n",
       "   'email_json_to_words__stemming': False,\n",
       "   'logistic_reg__solver': 'liblinear',\n",
       "   'tfidf__use_idf': False,\n",
       "   'word_vect__ngram_range': (1, 2)},\n",
       "  {'email_json_to_words__numberReplacement': True,\n",
       "   'email_json_to_words__stemming': False,\n",
       "   'logistic_reg__solver': 'lbfgs',\n",
       "   'tfidf__use_idf': True,\n",
       "   'word_vect__ngram_range': (1, 1)},\n",
       "  {'email_json_to_words__numberReplacement': True,\n",
       "   'email_json_to_words__stemming': False,\n",
       "   'logistic_reg__solver': 'lbfgs',\n",
       "   'tfidf__use_idf': True,\n",
       "   'word_vect__ngram_range': (1, 2)},\n",
       "  {'email_json_to_words__numberReplacement': True,\n",
       "   'email_json_to_words__stemming': False,\n",
       "   'logistic_reg__solver': 'lbfgs',\n",
       "   'tfidf__use_idf': False,\n",
       "   'word_vect__ngram_range': (1, 1)},\n",
       "  {'email_json_to_words__numberReplacement': True,\n",
       "   'email_json_to_words__stemming': False,\n",
       "   'logistic_reg__solver': 'lbfgs',\n",
       "   'tfidf__use_idf': False,\n",
       "   'word_vect__ngram_range': (1, 2)},\n",
       "  {'email_json_to_words__numberReplacement': False,\n",
       "   'email_json_to_words__stemming': True,\n",
       "   'logistic_reg__solver': 'liblinear',\n",
       "   'tfidf__use_idf': True,\n",
       "   'word_vect__ngram_range': (1, 1)},\n",
       "  {'email_json_to_words__numberReplacement': False,\n",
       "   'email_json_to_words__stemming': True,\n",
       "   'logistic_reg__solver': 'liblinear',\n",
       "   'tfidf__use_idf': True,\n",
       "   'word_vect__ngram_range': (1, 2)},\n",
       "  {'email_json_to_words__numberReplacement': False,\n",
       "   'email_json_to_words__stemming': True,\n",
       "   'logistic_reg__solver': 'liblinear',\n",
       "   'tfidf__use_idf': False,\n",
       "   'word_vect__ngram_range': (1, 1)},\n",
       "  {'email_json_to_words__numberReplacement': False,\n",
       "   'email_json_to_words__stemming': True,\n",
       "   'logistic_reg__solver': 'liblinear',\n",
       "   'tfidf__use_idf': False,\n",
       "   'word_vect__ngram_range': (1, 2)},\n",
       "  {'email_json_to_words__numberReplacement': False,\n",
       "   'email_json_to_words__stemming': True,\n",
       "   'logistic_reg__solver': 'lbfgs',\n",
       "   'tfidf__use_idf': True,\n",
       "   'word_vect__ngram_range': (1, 1)},\n",
       "  {'email_json_to_words__numberReplacement': False,\n",
       "   'email_json_to_words__stemming': True,\n",
       "   'logistic_reg__solver': 'lbfgs',\n",
       "   'tfidf__use_idf': True,\n",
       "   'word_vect__ngram_range': (1, 2)},\n",
       "  {'email_json_to_words__numberReplacement': False,\n",
       "   'email_json_to_words__stemming': True,\n",
       "   'logistic_reg__solver': 'lbfgs',\n",
       "   'tfidf__use_idf': False,\n",
       "   'word_vect__ngram_range': (1, 1)},\n",
       "  {'email_json_to_words__numberReplacement': False,\n",
       "   'email_json_to_words__stemming': True,\n",
       "   'logistic_reg__solver': 'lbfgs',\n",
       "   'tfidf__use_idf': False,\n",
       "   'word_vect__ngram_range': (1, 2)},\n",
       "  {'email_json_to_words__numberReplacement': False,\n",
       "   'email_json_to_words__stemming': False,\n",
       "   'logistic_reg__solver': 'liblinear',\n",
       "   'tfidf__use_idf': True,\n",
       "   'word_vect__ngram_range': (1, 1)},\n",
       "  {'email_json_to_words__numberReplacement': False,\n",
       "   'email_json_to_words__stemming': False,\n",
       "   'logistic_reg__solver': 'liblinear',\n",
       "   'tfidf__use_idf': True,\n",
       "   'word_vect__ngram_range': (1, 2)},\n",
       "  {'email_json_to_words__numberReplacement': False,\n",
       "   'email_json_to_words__stemming': False,\n",
       "   'logistic_reg__solver': 'liblinear',\n",
       "   'tfidf__use_idf': False,\n",
       "   'word_vect__ngram_range': (1, 1)},\n",
       "  {'email_json_to_words__numberReplacement': False,\n",
       "   'email_json_to_words__stemming': False,\n",
       "   'logistic_reg__solver': 'liblinear',\n",
       "   'tfidf__use_idf': False,\n",
       "   'word_vect__ngram_range': (1, 2)},\n",
       "  {'email_json_to_words__numberReplacement': False,\n",
       "   'email_json_to_words__stemming': False,\n",
       "   'logistic_reg__solver': 'lbfgs',\n",
       "   'tfidf__use_idf': True,\n",
       "   'word_vect__ngram_range': (1, 1)},\n",
       "  {'email_json_to_words__numberReplacement': False,\n",
       "   'email_json_to_words__stemming': False,\n",
       "   'logistic_reg__solver': 'lbfgs',\n",
       "   'tfidf__use_idf': True,\n",
       "   'word_vect__ngram_range': (1, 2)},\n",
       "  {'email_json_to_words__numberReplacement': False,\n",
       "   'email_json_to_words__stemming': False,\n",
       "   'logistic_reg__solver': 'lbfgs',\n",
       "   'tfidf__use_idf': False,\n",
       "   'word_vect__ngram_range': (1, 1)},\n",
       "  {'email_json_to_words__numberReplacement': False,\n",
       "   'email_json_to_words__stemming': False,\n",
       "   'logistic_reg__solver': 'lbfgs',\n",
       "   'tfidf__use_idf': False,\n",
       "   'word_vect__ngram_range': (1, 2)}],\n",
       " 'split0_test_score': array([0.97826087, 0.97342995, 0.96980676, 0.97101449, 0.97826087,\n",
       "        0.97342995, 0.96980676, 0.97101449, 0.97705314, 0.97826087,\n",
       "        0.96859903, 0.96980676, 0.97705314, 0.97826087, 0.96859903,\n",
       "        0.97101449, 0.98067633, 0.97705314, 0.97463768, 0.97463768,\n",
       "        0.98067633, 0.97705314, 0.97463768, 0.97342995, 0.98067633,\n",
       "        0.9794686 , 0.97342995, 0.97463768, 0.98067633, 0.9794686 ,\n",
       "        0.97342995, 0.97463768]),\n",
       " 'split1_test_score': array([0.97705314, 0.98067633, 0.96859903, 0.97222222, 0.97705314,\n",
       "        0.98067633, 0.96859903, 0.97222222, 0.97826087, 0.98309179,\n",
       "        0.97101449, 0.97342995, 0.97826087, 0.98309179, 0.97101449,\n",
       "        0.97342995, 0.9794686 , 0.98188406, 0.96980676, 0.97342995,\n",
       "        0.9794686 , 0.98188406, 0.96980676, 0.97342995, 0.98067633,\n",
       "        0.98067633, 0.96980676, 0.97101449, 0.98067633, 0.98067633,\n",
       "        0.96980676, 0.97101449]),\n",
       " 'split2_test_score': array([0.98065296, 0.97218863, 0.97218863, 0.97460701, 0.98065296,\n",
       "        0.97218863, 0.97218863, 0.97460701, 0.98307134, 0.98186215,\n",
       "        0.9758162 , 0.97460701, 0.98307134, 0.98186215, 0.97339782,\n",
       "        0.97460701, 0.98307134, 0.97460701, 0.97097944, 0.97460701,\n",
       "        0.98428053, 0.97460701, 0.97097944, 0.97460701, 0.98548972,\n",
       "        0.98065296, 0.97460701, 0.97823458, 0.98548972, 0.98065296,\n",
       "        0.97339782, 0.97823458]),\n",
       " 'split3_test_score': array([0.97218863, 0.97702539, 0.96251511, 0.96614268, 0.97218863,\n",
       "        0.97702539, 0.96251511, 0.96614268, 0.9758162 , 0.97702539,\n",
       "        0.96614268, 0.97218863, 0.9758162 , 0.97702539, 0.96614268,\n",
       "        0.97218863, 0.97702539, 0.9758162 , 0.96614268, 0.97097944,\n",
       "        0.97702539, 0.9758162 , 0.96614268, 0.97097944, 0.97944377,\n",
       "        0.97702539, 0.96493349, 0.96735187, 0.97944377, 0.97702539,\n",
       "        0.96493349, 0.96735187]),\n",
       " 'split4_test_score': array([0.98186215, 0.97944377, 0.97460701, 0.96735187, 0.98186215,\n",
       "        0.97944377, 0.97460701, 0.96735187, 0.98307134, 0.97823458,\n",
       "        0.97460701, 0.97460701, 0.98307134, 0.97823458, 0.97460701,\n",
       "        0.97460701, 0.98669891, 0.97823458, 0.97339782, 0.97097944,\n",
       "        0.98669891, 0.97823458, 0.97339782, 0.97097944, 0.98548972,\n",
       "        0.97460701, 0.97702539, 0.97460701, 0.98548972, 0.97460701,\n",
       "        0.9758162 , 0.97460701]),\n",
       " 'mean_test_score': array([0.97800355, 0.97655282, 0.96954331, 0.97026766, 0.97800355,\n",
       "        0.97655282, 0.96954331, 0.97026766, 0.97945458, 0.97969496,\n",
       "        0.97123589, 0.97292788, 0.97945458, 0.97969496, 0.97075221,\n",
       "        0.97316942, 0.98138811, 0.977519  , 0.97099288, 0.97292671,\n",
       "        0.98162995, 0.977519  , 0.97099288, 0.97268516, 0.98235517,\n",
       "        0.97848606, 0.97196052, 0.97316913, 0.98235517, 0.97848606,\n",
       "        0.97147685, 0.97316913]),\n",
       " 'std_test_score': array([0.00336722, 0.00329796, 0.0040742 , 0.00312171, 0.00336722,\n",
       "        0.00329796, 0.0040742 , 0.00312171, 0.00305259, 0.00234738,\n",
       "        0.00361349, 0.00179883, 0.00305259, 0.00234738, 0.0030923 ,\n",
       "        0.00140049, 0.00329431, 0.00249648, 0.00296647, 0.0016485 ,\n",
       "        0.00344972, 0.00249648, 0.00296647, 0.00145752, 0.00259862,\n",
       "        0.00235111, 0.00421506, 0.00369773, 0.00259862, 0.00235111,\n",
       "        0.00379314, 0.00369773]),\n",
       " 'rank_test_score': array([11, 15, 31, 29, 11, 15, 31, 29,  7,  5, 25, 20,  7,  5, 28, 17,  4,\n",
       "        13, 26, 21,  3, 13, 26, 22,  1,  9, 23, 18,  1,  9, 24, 18],\n",
       "       dtype=int32)}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_clf.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'email_json_to_words__numberReplacement': False,\n",
       " 'email_json_to_words__stemming': False,\n",
       " 'logistic_reg__solver': 'liblinear',\n",
       " 'tfidf__use_idf': True,\n",
       " 'word_vect__ngram_range': (1, 1)}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_clf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
