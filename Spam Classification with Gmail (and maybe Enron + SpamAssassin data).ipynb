{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['spam',\n",
       " 'hard_ham',\n",
       " 'spam_2',\n",
       " '.DS_Store',\n",
       " 'enron1',\n",
       " 'easy_ham',\n",
       " 'spam_assassin_ham',\n",
       " 'gmail_spam_examples',\n",
       " 'glove.6B']"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# helpful pre-reads:\n",
    "# https://www.kaggle.com/veleon/spam-classification\n",
    "# https://stackabuse.com/text-classification-with-python-and-scikit-learn\n",
    "# https://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html\n",
    "\n",
    "# Steps taken below:\n",
    "# Convert email into feature vector\n",
    "# Create Test & Training Set\n",
    "# Add Hyperparameters to:\n",
    "# - Strip email headers\n",
    "# - Convert to lowercase\n",
    "# - Remove punctuation\n",
    "# - Replace urls with \"URL\"\n",
    "# - Replace numbers with \"NUMBER\"\n",
    "# - Perform Stemming (trim word endings with nltk library)\n",
    "\n",
    "import os\n",
    "import re\n",
    "import email\n",
    "import email.policy\n",
    "from typing import Any\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "import urlextract\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn import model_selection\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.metrics import precision_score, recall_score, classification_report, confusion_matrix\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "os.listdir('data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amount of ham files: 2750\n",
      "Amount of spam files: 668\n",
      "Spam to Ham Ratio: 0.2429090909090909\n"
     ]
    }
   ],
   "source": [
    "ham_filenames = [name for name in sorted(os.listdir('data/spam_assassin_ham')) if len(name) > 20]\n",
    "spam_filenames = [name for name in sorted(os.listdir('data/gmail_spam_examples/converted')) if len(name) > 3]\n",
    "\n",
    "print('Amount of ham files:', len(ham_filenames))\n",
    "print('Amount of spam files:', len(spam_filenames))    \n",
    "print('Spam to Ham Ratio:',len(spam_filenames) / len(ham_filenames))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Object Type: <class 'str'>\n",
      "Subject: Get a cash offer for your home now\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def load_email(filename):\n",
    "    directory = \"data/spam_assassin_ham\"\n",
    "    with open(os.path.join(directory, filename), \"rb\") as f:\n",
    "        return email.parser.BytesParser(policy=email.policy.default).parse(f)\n",
    "    \n",
    "def load_file(filename):\n",
    "    directory = \"data/gmail_spam_examples/converted\"\n",
    "    # http://python-notes.curiousefficiency.org/en/latest/python3/text_file_processing.html\n",
    "    # latin-1 encoding because some of the spam files have retarded characters\n",
    "    with open(os.path.join(directory, filename)) as f:\n",
    "#         lines = f.readlines()\n",
    "        return f.read()\n",
    "        # like this one\n",
    "#         if filename.startswith(\"0754\"):\n",
    "#             print(lines)\n",
    "#         return {\n",
    "#             'subject': lines[0],\n",
    "#             'body': lines[1:],\n",
    "#         }\n",
    "    \n",
    "ham_emails = [load_email(filename=name) for name in ham_filenames]\n",
    "spam_emails = [load_file(filename=name) for name in spam_filenames]\n",
    "    \n",
    "    \n",
    "testEmail = spam_emails[0]\n",
    "\n",
    "# print('Header Field Names:', testEmail.keys())\n",
    "# print('\\n\\n')\n",
    "# print('Message Field Values:', testEmail.values())\n",
    "# print('\\n\\n')\n",
    "print('Object Type:', type(testEmail))\n",
    "print(testEmail)\n",
    "\n",
    "# if testEmail.is_multipart():\n",
    "#     for payload in testEmail.get_payload():\n",
    "#         # if payload.is_multipart(): ...\n",
    "#         print('payload:', payload.get_payload())\n",
    "# else:\n",
    "#     print(testEmail.get_payload())\n",
    "# print('Message Content:', testEmail.get_content())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def html_to_plain(email):\n",
    "    try:\n",
    "        soup = BeautifulSoup(email, 'html.parser')\n",
    "#         print(type(soup.text))\n",
    "        return \" \".join(soup.text.split())\n",
    "#         return soup.text.replace('\\n', '').replace('\\t', '')\n",
    "    except:\n",
    "        return \"something went wrong parsing html text\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Subject: Get a cash offer for your home now'"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "html_to_plain(testEmail)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On Thu, Aug 22, 2002 at 05:13:01PM +0100, Fergal Moran mentioned:\n",
      "> In a nutshell - Solaris is Suns own flavour of UNIX.\n",
      "\n",
      " Though I'm sure that this nice person would like a bit more detail.\n",
      "\n",
      " Solaris is quite different to Linux, though these days you can make\n",
      "solaris act a lot like linux with an extra CD of GNU tools Sun ship with\n",
      "solaris. It is based on the SysV unix family, so it's quite similar to\n",
      "other unixen like HPUX and SCO.\n",
      "\n",
      " Sun's hardware in general is more reliable, and a lot more expensive. One\n",
      "of the main bonuses you get by buying Sun is that you are getting your\n",
      "hardware and software from one company, so if you have a support contract,\n",
      "they have to fix it. They can't fob you off with 'that's a software\n",
      "problem, talk to the software vendor.' etc.\n",
      "\n",
      " If you are set on Linux, you most likely can do your own support. There\n",
      "is then a world of different hardware options. You can run Linux on Sparc,\n",
      "though some companies like RedHat don't maintain a sparc port anymore.\n",
      "\n",
      " You can also buy your machine from linux-oriented companies like DNUK,\n",
      "who do machines designed to run linux, and their own version of linux,\n",
      "that has a few extras for their machines. Or, you can get a machine from a\n",
      "cheaper company like Dell, and it'll most likely work, most of the time.\n",
      "\n",
      "John\n",
      "\n",
      "\n",
      "-- \n",
      "Irish Linux Users' Group: ilug@linux.ie\n",
      "http://www.linux.ie/mailman/listinfo/ilug for (un)subscription information.\n",
      "List maintainer: listmaster@linux.ie\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Re: [ILUG] Sun Solaris..'"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_email_structure(email):    \n",
    "    payload = email.get_payload()\n",
    "    if isinstance(payload, list):\n",
    "        return \"multipart({})\".format(\", \".join([\n",
    "            get_email_structure(sub_email)\n",
    "            for sub_email in payload\n",
    "        ]))\n",
    "    else:\n",
    "        return email.get_content_type()\n",
    "    \n",
    "def email_to_plain(email):\n",
    "    struct = get_email_structure(email)\n",
    "    for part in email.walk():\n",
    "        partContentType = part.get_content_type()\n",
    "        if partContentType not in ['text/plain','text/html']:\n",
    "            continue\n",
    "        try:\n",
    "            partContent = part.get_content()\n",
    "        except: # in case of encoding issues\n",
    "            partContent = str(part.get_payload())\n",
    "        if partContentType == 'text/plain':\n",
    "            return partContent\n",
    "        else:\n",
    "            return html_to_plain(part)\n",
    "\n",
    "print(email_to_plain(ham_emails[42]))\n",
    "ham_emails[42]['subject']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "ham_emails = [(ham['subject'] or '') + '\\n' + email_to_plain(ham) for ham in ham_emails]\n",
    "spam_emails = [html_to_plain(spam) for spam in spam_emails]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "snowball_stemmer = SnowballStemmer('english')\n",
    "\n",
    "class EmailTextToWords(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, lowercaseConversion = True, punctuationRemoval = True, \n",
    "                 urlReplacement = True, numberReplacement = False, stemming = False):\n",
    "        self.lowercaseConversion = lowercaseConversion\n",
    "        self.punctuationRemoval = punctuationRemoval\n",
    "        self.urlReplacement = urlReplacement\n",
    "        self.urlExtractor = urlextract.URLExtract()\n",
    "        self.numberReplacement = numberReplacement\n",
    "        self.stemming = stemming\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X, y=None):\n",
    "        X_to_words = []\n",
    "        for text in X:\n",
    "            if text is None:\n",
    "                text = 'empty'\n",
    "            if self.lowercaseConversion:\n",
    "                text = text.lower()\n",
    "                \n",
    "            if self.urlReplacement:\n",
    "                urls = self.urlExtractor.find_urls(text)\n",
    "                for url in urls:\n",
    "                   text = text.replace(url, 'URL')   \n",
    "            \n",
    "            # apparently removing numbers helped\n",
    "            if self.numberReplacement:\n",
    "                text = re.sub('\\d', '%d', text)\n",
    "                    \n",
    "            if self.punctuationRemoval:\n",
    "                text = text.replace('.','')\n",
    "                text = text.replace(',','')\n",
    "                text = text.replace('!','')\n",
    "                text = text.replace('?','')\n",
    "                \n",
    "            if self.stemming:\n",
    "                words = text.split(' ')\n",
    "                \n",
    "                stemmed_words = []\n",
    "                for word in words:\n",
    "                    stemmed_words.append(snowball_stemmer.stem(word))\n",
    "                \n",
    "                text = ' '.join(stemmed_words)\n",
    "            \n",
    "            X_to_words.append(text)\n",
    "        return np.array(X_to_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<3x582 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 651 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_few = ham_emails[:3]\n",
    "X_few_text = EmailTextToWords().fit_transform(X_few)\n",
    "vocab_transformer = CountVectorizer()\n",
    "X_few_vectors = vocab_transformer.fit_transform(X_few_text)\n",
    "X_few_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'re': 421,\n",
       " 'new': 364,\n",
       " 'sequences': 463,\n",
       " 'window': 568,\n",
       " 'date': 143,\n",
       " 'wed': 559,\n",
       " '21': 21,\n",
       " 'aug': 73,\n",
       " '2002': 20,\n",
       " '10': 6,\n",
       " '54': 32,\n",
       " '46': 27,\n",
       " '0500': 5,\n",
       " 'from': 216,\n",
       " 'chris': 112,\n",
       " 'garrigues': 224,\n",
       " 'cwg': 140,\n",
       " 'dated': 144,\n",
       " '103037728706fa6d': 8,\n",
       " 'deepeddycom': 151,\n",
       " 'message': 341,\n",
       " 'id': 268,\n",
       " '10299452874797tmda': 7,\n",
       " 'deepeddyvirciocom': 152,\n",
       " 'can': 99,\n",
       " 'reproduce': 435,\n",
       " 'this': 517,\n",
       " 'error': 183,\n",
       " 'for': 211,\n",
       " 'me': 336,\n",
       " 'it': 286,\n",
       " 'is': 283,\n",
       " 'very': 548,\n",
       " 'repeatable': 431,\n",
       " 'like': 307,\n",
       " 'every': 184,\n",
       " 'time': 519,\n",
       " 'without': 570,\n",
       " 'fail': 196,\n",
       " 'the': 510,\n",
       " 'debug': 149,\n",
       " 'log': 317,\n",
       " 'of': 374,\n",
       " 'pick': 399,\n",
       " 'happening': 245,\n",
       " '18': 14,\n",
       " '19': 16,\n",
       " '03': 3,\n",
       " 'pick_it': 400,\n",
       " 'exec': 188,\n",
       " 'inbox': 274,\n",
       " 'list': 310,\n",
       " 'lbrace': 300,\n",
       " 'subject': 492,\n",
       " 'ftp': 219,\n",
       " 'rbrace': 420,\n",
       " '4852': 30,\n",
       " 'sequence': 462,\n",
       " 'mercury': 340,\n",
       " '04': 4,\n",
       " 'ftoc_pickmsgs': 218,\n",
       " 'hit': 257,\n",
       " 'marking': 332,\n",
       " 'hits': 258,\n",
       " 'tkerror': 523,\n",
       " 'syntax': 497,\n",
       " 'in': 273,\n",
       " 'expression': 193,\n",
       " 'int': 279,\n",
       " 'note': 371,\n",
       " 'if': 271,\n",
       " 'run': 447,\n",
       " 'command': 118,\n",
       " 'by': 96,\n",
       " 'hand': 242,\n",
       " 'delta': 155,\n",
       " 'that': 509,\n",
       " 'where': 565,\n",
       " 'comes': 116,\n",
       " 'obviously': 373,\n",
       " 'version': 547,\n",
       " 'nmh': 368,\n",
       " 'using': 543,\n",
       " '104': 9,\n",
       " 'compiled': 124,\n",
       " 'on': 377,\n",
       " 'url': 541,\n",
       " 'at': 71,\n",
       " 'sun': 494,\n",
       " 'mar': 328,\n",
       " '17': 12,\n",
       " '14': 11,\n",
       " '55': 33,\n",
       " '56': 34,\n",
       " 'ict': 267,\n",
       " 'and': 62,\n",
       " 'relevant': 429,\n",
       " 'part': 392,\n",
       " 'my': 361,\n",
       " 'mh_profile': 342,\n",
       " 'mhparam': 343,\n",
       " 'seq': 461,\n",
       " 'sel': 458,\n",
       " 'since': 470,\n",
       " 'works': 572,\n",
       " 'actually': 45,\n",
       " 'both': 87,\n",
       " 'them': 511,\n",
       " 'one': 378,\n",
       " 'explicit': 192,\n",
       " 'line': 309,\n",
       " 'search': 454,\n",
       " 'popup': 405,\n",
       " 'do': 163,\n",
       " 'get': 225,\n",
       " 'created': 133,\n",
       " 'kre': 298,\n",
       " 'ps': 415,\n",
       " 'still': 486,\n",
       " 'code': 115,\n",
       " 'form': 212,\n",
       " 'day': 147,\n",
       " 'ago': 51,\n",
       " 'haven': 247,\n",
       " 'been': 79,\n",
       " 'able': 41,\n",
       " 'to': 525,\n",
       " 'reach': 422,\n",
       " 'cvs': 139,\n",
       " 'repository': 434,\n",
       " 'today': 526,\n",
       " 'local': 316,\n",
       " 'routing': 446,\n",
       " 'issue': 285,\n",
       " 'think': 516,\n",
       " '_______________________________________________': 39,\n",
       " 'exmh': 190,\n",
       " 'workers': 571,\n",
       " 'mailing': 324,\n",
       " 'redhatcom': 426,\n",
       " 'personal': 397,\n",
       " 'finance': 203,\n",
       " 'resolutions': 439,\n",
       " 'you': 579,\n",
       " 'keep': 295,\n",
       " 'motley': 354,\n",
       " 'fool': 209,\n",
       " 'wednesday': 560,\n",
       " 'january': 288,\n",
       " 'mkettler': 346,\n",
       " 'homecom': 260,\n",
       " 'ask': 69,\n",
       " 'stop': 489,\n",
       " 'solicitation': 472,\n",
       " 'your': 580,\n",
       " 'money': 350,\n",
       " 'spotlight': 480,\n",
       " 'should': 469,\n",
       " 'all': 56,\n",
       " 'savings': 451,\n",
       " 'be': 77,\n",
       " 'market': 330,\n",
       " 'community': 121,\n",
       " 'tips': 521,\n",
       " 'least': 301,\n",
       " 'thing': 515,\n",
       " 'differently': 158,\n",
       " 'sponsored': 479,\n",
       " 'datek': 145,\n",
       " 'online': 380,\n",
       " 'built': 90,\n",
       " 'trade': 529,\n",
       " 'proprietary': 414,\n",
       " 'auto': 74,\n",
       " 'technology': 502,\n",
       " '999': 38,\n",
       " 'commission': 119,\n",
       " 'equity': 181,\n",
       " 'trades': 530,\n",
       " '60': 35,\n",
       " 'second': 455,\n",
       " 'execution': 189,\n",
       " 'commitment': 120,\n",
       " 'apply': 65,\n",
       " 'account': 43,\n",
       " 'now': 372,\n",
       " 'tired': 522,\n",
       " 'getting': 226,\n",
       " 'so': 471,\n",
       " 'many': 327,\n",
       " 'credit': 134,\n",
       " 'card': 101,\n",
       " 'offers': 376,\n",
       " 'mail': 322,\n",
       " 'already': 58,\n",
       " 'have': 246,\n",
       " 'don': 166,\n",
       " 'want': 553,\n",
       " 'any': 64,\n",
       " 'more': 352,\n",
       " 'offering': 375,\n",
       " 'ones': 379,\n",
       " 'how': 265,\n",
       " 'there': 512,\n",
       " 'are': 66,\n",
       " 'three': 518,\n",
       " 'main': 325,\n",
       " 'bureaus': 92,\n",
       " 'united': 539,\n",
       " 'states': 485,\n",
       " 'they': 514,\n",
       " 've': 546,\n",
       " 'agreed': 52,\n",
       " 'someone': 474,\n",
       " 'contacts': 125,\n",
       " 'asks': 70,\n",
       " 'removed': 430,\n",
       " 'junk': 293,\n",
       " 'er': 182,\n",
       " 'direct': 160,\n",
       " 'lists': 311,\n",
       " 'telemarketing': 503,\n",
       " 'phone': 398,\n",
       " 'll': 315,\n",
       " 'honor': 262,\n",
       " 'request': 436,\n",
       " 'full': 220,\n",
       " 'answer': 63,\n",
       " 'let': 306,\n",
       " 'face': 194,\n",
       " 'year': 577,\n",
       " 'mostly': 353,\n",
       " 'nightmares': 367,\n",
       " 'hang': 243,\n",
       " 'over': 387,\n",
       " 'our': 385,\n",
       " 'heads': 249,\n",
       " 'dark': 142,\n",
       " 'cloud': 114,\n",
       " 'follows': 208,\n",
       " 'eeyore': 174,\n",
       " 'everywhere': 186,\n",
       " 'he': 248,\n",
       " 'goes': 229,\n",
       " 'we': 557,\n",
       " 'because': 78,\n",
       " 'too': 528,\n",
       " 'vague': 544,\n",
       " 'imply': 272,\n",
       " 'guilt': 239,\n",
       " 'inducing': 277,\n",
       " 'judgment': 292,\n",
       " 'live': 313,\n",
       " 'moment': 348,\n",
       " 'good': 231,\n",
       " 'person': 396,\n",
       " 'lose': 319,\n",
       " 'lot': 320,\n",
       " 'weight': 562,\n",
       " 'no': 369,\n",
       " 'pressure': 409,\n",
       " 'right': 444,\n",
       " 'wrong': 575,\n",
       " 'despair': 156,\n",
       " 'put': 417,\n",
       " 'together': 527,\n",
       " 'quick': 418,\n",
       " 'proactive': 411,\n",
       " 'doable': 164,\n",
       " 'resolution': 438,\n",
       " 'save': 449,\n",
       " 'big': 83,\n",
       " 'bucks': 89,\n",
       " 'comfortably': 117,\n",
       " 'future': 222,\n",
       " 'behind': 80,\n",
       " 'started': 483,\n",
       " 'dial': 157,\n",
       " 'away': 75,\n",
       " 'debt': 148,\n",
       " 'true': 532,\n",
       " 'lower': 321,\n",
       " 'interest': 280,\n",
       " 'rates': 419,\n",
       " 'with': 569,\n",
       " 'call': 97,\n",
       " 'stash': 484,\n",
       " 'short': 467,\n",
       " 'term': 506,\n",
       " 'cash': 103,\n",
       " 'under': 538,\n",
       " 'mattress': 335,\n",
       " 'earn': 171,\n",
       " 'higher': 255,\n",
       " 'yields': 578,\n",
       " 'cds': 104,\n",
       " 'markets': 331,\n",
       " 'invest': 281,\n",
       " 'less': 305,\n",
       " 'make': 326,\n",
       " 'sure': 496,\n",
       " 'broker': 88,\n",
       " 'isn': 284,\n",
       " 'charging': 108,\n",
       " 'much': 359,\n",
       " 'compare': 123,\n",
       " 'hire': 256,\n",
       " 'only': 381,\n",
       " 'seconds': 456,\n",
       " 'retire': 442,\n",
       " 'early': 170,\n",
       " 'thanks': 508,\n",
       " 'recent': 424,\n",
       " 'tax': 500,\n",
       " 'law': 299,\n",
       " 'changes': 107,\n",
       " 'start': 482,\n",
       " 'pitching': 401,\n",
       " 'retirement': 443,\n",
       " 'accounts': 44,\n",
       " 'refinance': 427,\n",
       " 'when': 564,\n",
       " 'drop': 168,\n",
       " 'some': 473,\n",
       " 'homeowners': 261,\n",
       " 'opportunity': 382,\n",
       " 'seems': 457,\n",
       " 'daunting': 146,\n",
       " 'go': 228,\n",
       " 'alone': 57,\n",
       " 'unbiased': 537,\n",
       " 'professional': 413,\n",
       " 'advice': 48,\n",
       " 'about': 42,\n",
       " 'finances': 204,\n",
       " 'tmf': 524,\n",
       " 'advisor': 49,\n",
       " 'fact': 195,\n",
       " 'excited': 187,\n",
       " 'service': 464,\n",
       " 'free': 214,\n",
       " 'month': 351,\n",
       " 'trial': 531,\n",
       " 'subscription': 493,\n",
       " 'information': 278,\n",
       " 'here': 253,\n",
       " 'saving': 450,\n",
       " 'aid': 53,\n",
       " 'journey': 290,\n",
       " 'financial': 205,\n",
       " 'independence': 276,\n",
       " 'visit': 549,\n",
       " 'area': 67,\n",
       " 'um': 536,\n",
       " 'supporters': 495,\n",
       " 'idea': 269,\n",
       " 'long': 318,\n",
       " 'stock': 487,\n",
       " 'but': 93,\n",
       " 'volatile': 550,\n",
       " 'buying': 95,\n",
       " 'stocks': 488,\n",
       " 'gain': 223,\n",
       " 'really': 423,\n",
       " 'speculating': 477,\n",
       " 'not': 370,\n",
       " 'investing': 282,\n",
       " 'would': 574,\n",
       " 'wanted': 554,\n",
       " 'emergency': 179,\n",
       " 'fund': 221,\n",
       " 'or': 383,\n",
       " 'house': 264,\n",
       " 'down': 167,\n",
       " 'payment': 394,\n",
       " 'kid': 297,\n",
       " 'tuition': 534,\n",
       " 'nasdaq': 363,\n",
       " 'its': 287,\n",
       " 'height': 251,\n",
       " 'march': 329,\n",
       " '2000': 18,\n",
       " 'afford': 50,\n",
       " 'wait': 551,\n",
       " 'out': 386,\n",
       " '30': 24,\n",
       " 'value': 545,\n",
       " 'course': 130,\n",
       " 'just': 294,\n",
       " 'going': 230,\n",
       " 'leave': 302,\n",
       " 'hanging': 244,\n",
       " 'figure': 201,\n",
       " 'best': 82,\n",
       " 'place': 402,\n",
       " 'nifty': 365,\n",
       " 'center': 105,\n",
       " 'folks': 207,\n",
       " 'living': 314,\n",
       " 'below': 81,\n",
       " 'means': 338,\n",
       " 'discussion': 161,\n",
       " 'board': 84,\n",
       " 'sharing': 466,\n",
       " 'am': 59,\n",
       " 'carry': 102,\n",
       " 'coupon': 128,\n",
       " 'envelope': 180,\n",
       " 'purse': 416,\n",
       " 'find': 206,\n",
       " 'myself': 362,\n",
       " 'grocery': 235,\n",
       " 'store': 490,\n",
       " 'bunch': 91,\n",
       " 'stuff': 491,\n",
       " 'which': 566,\n",
       " 'could': 127,\n",
       " 'hadn': 241,\n",
       " 'left': 303,\n",
       " 'coupons': 129,\n",
       " 'home': 259,\n",
       " 'reimbursed': 428,\n",
       " 'dime': 159,\n",
       " 'company': 122,\n",
       " 'owes': 388,\n",
       " 'bad': 76,\n",
       " 'habit': 240,\n",
       " 'filing': 202,\n",
       " 'expense': 191,\n",
       " 'reports': 433,\n",
       " 'little': 312,\n",
       " 'miles': 345,\n",
       " 'mileage': 344,\n",
       " 'mediation': 339,\n",
       " 'ten': 505,\n",
       " '310': 25,\n",
       " 'times': 520,\n",
       " 'than': 507,\n",
       " 'definitely': 153,\n",
       " 'worth': 573,\n",
       " 'an': 61,\n",
       " 'report': 432,\n",
       " 'crock': 135,\n",
       " 'pot': 408,\n",
       " 'meal': 337,\n",
       " 'week': 561,\n",
       " 'tempted': 504,\n",
       " 'buy': 94,\n",
       " 'something': 475,\n",
       " 'eat': 173,\n",
       " 'fridays': 215,\n",
       " 'great': 233,\n",
       " 'either': 176,\n",
       " 'cut': 138,\n",
       " 'distance': 162,\n",
       " 'calls': 98,\n",
       " 'else': 177,\n",
       " 'cheaper': 110,\n",
       " 'plan': 403,\n",
       " 'document': 165,\n",
       " 'everything': 185,\n",
       " 'give': 227,\n",
       " 'charity': 109,\n",
       " 'including': 275,\n",
       " 'clothes': 113,\n",
       " 'books': 86,\n",
       " 'deduct': 150,\n",
       " 'taxes': 501,\n",
       " 'turning': 535,\n",
       " 'thermostat': 513,\n",
       " 'night': 366,\n",
       " 'humidifier': 266,\n",
       " 'moist': 347,\n",
       " 'air': 54,\n",
       " 'heat': 250,\n",
       " 'own': 389,\n",
       " 'add': 46,\n",
       " 'growing': 238,\n",
       " 'portfolio': 406,\n",
       " 'boards': 85,\n",
       " 'wanna': 552,\n",
       " 'take': 498,\n",
       " 'shortcut': 468,\n",
       " 'check': 111,\n",
       " 'crash': 132,\n",
       " 'courses': 131,\n",
       " 'fastest': 199,\n",
       " 'way': 556,\n",
       " 'order': 384,\n",
       " 'results': 441,\n",
       " 'hour': 263,\n",
       " 'try': 533,\n",
       " 'perks': 395,\n",
       " 'fools': 210,\n",
       " 'special': 476,\n",
       " 'customer': 137,\n",
       " 'staff': 481,\n",
       " 'help': 252,\n",
       " 'email': 178,\n",
       " 'delivery': 154,\n",
       " 'problems': 412,\n",
       " 'unsubscribe': 540,\n",
       " 'change': 106,\n",
       " 'settings': 465,\n",
       " '_____________________________________________________': 40,\n",
       " 'copyright': 126,\n",
       " '1995': 17,\n",
       " '2001': 19,\n",
       " 'rights': 445,\n",
       " 'reserved': 437,\n",
       " 'legal': 304,\n",
       " 'msgid': 358,\n",
       " 'msg': 357,\n",
       " '7678': 37,\n",
       " '01': 0,\n",
       " '02_13': 2,\n",
       " '22': 22,\n",
       " '47': 28,\n",
       " '4816731_25_plain_messageaddressmsg': 29,\n",
       " '13': 10,\n",
       " '51': 31,\n",
       " '39': 26,\n",
       " '02': 1,\n",
       " 'mailer': 323,\n",
       " 'sender': 460,\n",
       " 'masterv': 334,\n",
       " '184': 15,\n",
       " 'daemonv': 141,\n",
       " 'recipient': 425,\n",
       " 'zzzzteana': 581,\n",
       " 'alexander': 55,\n",
       " 'martin': 333,\n",
       " 'posted': 407,\n",
       " 'tassos': 499,\n",
       " 'papadopoulos': 390,\n",
       " 'greek': 234,\n",
       " 'sculptor': 452,\n",
       " 'judged': 291,\n",
       " 'limestone': 308,\n",
       " 'mount': 355,\n",
       " 'kerdylio': 296,\n",
       " '70': 36,\n",
       " 'east': 172,\n",
       " 'salonika': 448,\n",
       " 'far': 197,\n",
       " 'athos': 72,\n",
       " 'monastic': 349,\n",
       " 'was': 555,\n",
       " 'ideal': 270,\n",
       " 'patriotic': 393,\n",
       " 'sculpture': 453,\n",
       " 'as': 68,\n",
       " 'well': 563,\n",
       " 'granite': 232,\n",
       " 'features': 200,\n",
       " '240': 23,\n",
       " 'ft': 217,\n",
       " 'high': 254,\n",
       " '170': 13,\n",
       " 'wide': 567,\n",
       " 'museum': 360,\n",
       " 'restored': 440,\n",
       " 'amphitheatre': 60,\n",
       " 'car': 100,\n",
       " 'park': 391,\n",
       " 'admiring': 47,\n",
       " 'crowds': 136,\n",
       " 'planned': 404,\n",
       " 'mountain': 356,\n",
       " 'weather': 558,\n",
       " 'pretty': 410,\n",
       " 'fast': 198,\n",
       " 'yahoo': 576,\n",
       " 'groups': 237,\n",
       " 'sponsor': 478,\n",
       " 'dvds': 169,\n",
       " 'join': 289,\n",
       " 'group': 236,\n",
       " 'send': 459,\n",
       " 'forteana': 213,\n",
       " 'egroupscom': 175,\n",
       " 'use': 542}"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_transformer.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 771)\t1\n",
      "  (0, 987)\t1\n",
      "  (0, 1201)\t1\n",
      "  (0, 286)\t1\n",
      "  (0, 1189)\t1\n",
      "  (0, 56)\t1\n",
      "  (0, 148)\t1\n",
      "  (0, 49)\t2\n",
      "  (0, 15)\t1\n",
      "  (0, 80)\t1\n",
      "  (0, 69)\t1\n",
      "  (0, 13)\t1\n",
      "  (0, 219)\t1\n",
      "  (0, 462)\t1\n",
      "  (0, 280)\t1\n",
      "  (0, 288)\t1\n",
      "  (0, 23)\t1\n",
      "  (0, 304)\t1\n",
      "  (0, 718)\t1\n",
      "  (0, 557)\t1\n",
      "  (0, 21)\t1\n",
      "  (0, 306)\t1\n",
      "  (0, 916)\t1\n",
      "  (0, 369)\t2\n",
      "  (0, 908)\t1\n",
      "  :\t:\n",
      "  (2, 273)\t1\n",
      "  (2, 854)\t1\n",
      "  (2, 760)\t1\n",
      "  (2, 638)\t1\n",
      "  (2, 494)\t1\n",
      "  (2, 640)\t1\n",
      "  (2, 1188)\t1\n",
      "  (2, 866)\t1\n",
      "  (2, 405)\t1\n",
      "  (2, 1218)\t2\n",
      "  (2, 504)\t1\n",
      "  (2, 1013)\t1\n",
      "  (2, 341)\t1\n",
      "  (2, 442)\t1\n",
      "  (2, 590)\t1\n",
      "  (2, 1134)\t1\n",
      "  (2, 502)\t1\n",
      "  (2, 978)\t1\n",
      "  (2, 360)\t1\n",
      "  (2, 440)\t1\n",
      "  (2, 1133)\t1\n",
      "  (2, 353)\t1\n",
      "  (2, 1158)\t1\n",
      "  (2, 505)\t1\n",
      "  (2, 1045)\t1\n",
      "['01', '01 02_13', '02', '02 2002', '02_13', '02_13 22', '03', '03 exec', '03 pick_it', '04', '04 ftoc_pickmsgs', '04 marking', '04 tkerror', '0500', '0500 chris', '10', '10 2000', '10 54', '10 miles', '10 times', '10 try', '10299452874797tmda', '10299452874797tmda deepeddyvirciocom', '103037728706fa6d', '103037728706fa6d deepeddycom', '104', '104 compiled', '13', '13 51', '14', '14 55', '17', '17 14', '170', '170 ft', '18', '18 19', '184', '184 message', '184 version', '19', '19 03', '19 04', '1995', '1995 2001', '2000', '2000 afford', '2001', '2001 motley', '2002', '2002 01', '2002 10', '2002 mkettler', '2002 relevant', '2002 save', '2002 version', '21', '21 aug', '22', '22 47', '240', '240 ft', '30', '30 bucks', '30 drop', '310', '310 10', '39', '39 02', '46', '46 0500', '47', '47 4816731_25_plain_messageaddressmsg', '4816731_25_plain_messageaddressmsg', '4816731_25_plain_messageaddressmsg 13', '4852', '4852 4852', '4852 sequence', '51', '51 39', '54', '54 46', '55', '55 56', '56', '56 ict', '60', '60 second', '60 seconds', '70', '70 miles', '7678', '7678 2002', '999', '999 commission', '_______________________________________________', '_______________________________________________ exmh', '_____________________________________________________', '_____________________________________________________ copyright', 'able', 'able reach', 'account', 'account url', 'accounts', 'accounts url', 'actually', 'actually doable', 'actually one', 'add', 'add growing', 'admiring', 'admiring crowds', 'advice', 'advice finances', 'advisor', 'advisor fact', 'afford', 'afford wait', 'ago', 'ago able', 'agreed', 'agreed someone', 'aid', 'aid journey', 'air', 'air cheaper', 'alexander', 'alexander granite', 'alexander martin', 'alone', 'alone get', 'already', 'already card', 'amphitheatre', 'amphitheatre car', 'answer', 'answer url', 'apply', 'apply datek', 'area', 'area url', 'ask', 'ask fool', 'ask staff', 'asks', 'asks removed', 'athos', 'athos monastic', 'aug', 'aug 2002', 'auto', 'auto routing', 'away', 'away debt', 'bad', 'bad habit', 'behind', 'behind let', 'behind plan', 'best', 'best place', 'big', 'big bucks', 'big money', 'big supporters', 'board', 'board sharing', 'boards', 'boards url', 'books', 'books deduct', 'broker', 'broker charging', 'bucks', 'bucks definitely', 'bucks live', 'built', 'built trade', 'bunch', 'bunch money', 'bunch stuff', 'bureaus', 'bureaus united', 'buy', 'buy something', 'buying', 'buying bunch', 'buying stocks', 'call', 'call url', 'calls', 'calls else', 'car', 'car park', 'card', 'card interest', 'card offers', 'card want', 'carry', 'carry coupon', 'cash', 'cash savings', 'cds', 'cds money', 'center', 'center url', 'change', 'change settings', 'changes', 'changes start', 'charging', 'charging much', 'charity', 'charity including', 'cheaper', 'cheaper heat', 'cheaper long', 'check', 'check new', 'chris', 'chris garrigues', 'clothes', 'clothes books', 'cloud', 'cloud follows', 'code', 'code form', 'comes', 'comes mh_profile', 'comes obviously', 'comfortably', 'comfortably future', 'command', 'command hand', 'command line', 'command works', 'commission', 'commission online', 'commitment', 'commitment apply', 'community', 'community ideal', 'community tips', 'company', 'company owes', 'compare', 'compare rates', 'compiled', 'compiled url', 'contacts', 'contacts one', 'copyright', 'copyright 1995', 'could', 'could save', 'coupon', 'coupon envelope', 'coupons', 'coupons home', 'course', 'course going', 'courses', 'courses fastest', 'crash', 'crash courses', 'created', 'created kre', 'credit', 'credit bureaus', 'credit card', 'crock', 'crock pot', 'crowds', 'crowds planned', 'customer', 'customer service', 'cut', 'cut long', 'cvs', 'cvs repository', 'cwg', 'cwg dated', 'daemonv', 'daemonv 184', 'dark', 'dark cloud', 'date', 'date wed', 'dated', 'dated 103037728706fa6d', 'datek', 'datek account', 'datek datek', 'datek online', 'daunting', 'daunting want', 'day', 'day ago', 'debt', 'debt true', 'debug', 'debug log', 'deduct', 'deduct taxes', 'deepeddycom', 'deepeddycom message', 'deepeddyvirciocom', 'deepeddyvirciocom reproduce', 'definitely', 'definitely worth', 'delivery', 'delivery problems', 'delta', 'delta mhparam', 'delta pick', 'despair', 'despair put', 'dial', 'dial away', 'differently', 'differently 2002', 'differently folks', 'differently url', 'dime', 'dime company', 'direct', 'direct mail', 'discussion', 'discussion board', 'discussion boards', 'distance', 'distance phone', 'distance plan', 'doable', 'doable year', 'document', 'document everything', 'drop', 'drop homeowners', 'drop value', 'dvds', 'dvds free', 'early', 'early thanks', 'earn', 'earn higher', 'east', 'east salonika', 'eat', 'eat fridays', 'eeyore', 'eeyore everywhere', 'egroupscom', 'egroupscom use', 'either', 'either cut', 'else', 'else find', 'email', 'email delivery', 'email forteana', 'emergency', 'emergency fund', 'envelope', 'envelope purse', 'equity', 'equity trades', 'er', 'er direct', 'error', 'error expression', 'error repeatable', 'every', 'every dime', 'every moment', 'every time', 'everything', 'everything give', 'everywhere', 'everywhere goes', 'excited', 'excited service', 'exec', 'exec pick', 'execution', 'execution commitment', 'exmh', 'exmh workers', 'expense', 'expense report', 'expense reports', 'explicit', 'explicit command', 'expression', 'expression int', 'face', 'face new', 'fact', 'fact excited', 'fail', 'fail debug', 'fail keep', 'far', 'far mount', 'fast', 'fast yahoo', 'fastest', 'fastest way', 'features', 'features 240', 'figure', 'figure best', 'filing', 'filing expense', 'finance', 'finance area', 'finance resolutions', 'finance wednesday', 'finances', 'finances order', 'finances tmf', 'financial', 'financial independence', 'find', 'find cheaper', 'find grocery', 'folks', 'folks living', 'follows', 'follows eeyore', 'fool', 'fool personal', 'fool rights', 'fool stop', 'fool tired', 'fools', 'fools get', 'form', 'form day', 'forteana', 'forteana unsubscribe', 'free', 'free join', 'free one', 'free stuff', 'fridays', 'fridays would', 'ft', 'ft high', 'ft wide', 'ftoc_pickmsgs', 'ftoc_pickmsgs hit', 'ftp', 'ftp rbrace', 'full', 'full answer', 'fund', 'fund house', 'future', 'future resolution', 'gain', 'gain really', 'garrigues', 'garrigues cwg', 'get', 'get behind', 'get created', 'get finances', 'get free', 'get reimbursed', 'get results', 'get savings', 'get started', 'get tempted', 'get unbiased', 'getting', 'getting many', 'give', 'give charity', 'go', 'go alone', 'go buy', 'go mediation', 'go url', 'goes', 'goes mostly', 'going', 'going differently', 'going leave', 'going make', 'good', 'good person', 'granite', 'granite features', 'granite limestone', 'great', 'great time', 'greek', 'greek sculptor', 'grocery', 'grocery store', 'group', 'group send', 'groups', 'groups sponsor', 'groups subject', 'growing', 'growing list', 'guilt', 'guilt inducing', 'habit', 'habit filing', 'hand', 'hand delta', 'hang', 'hang heads', 'hanging', 'hanging figure', 'happening', 'happening 18', 'heads', 'heads like', 'heat', 'heat tips', 'height', 'height march', 'help', 'help email', 'high', 'high 170', 'higher', 'higher yields', 'hire', 'hire new', 'hit', 'hit 18', 'hit comes', 'hit hit', 'hits', 'hits 18', 'home', 'home make', 'homecom', 'homecom issue', 'homeowners', 'homeowners opportunity', 'honor', 'honor request', 'hour', 'hour 10', 'house', 'house payment', 'house run', 'humidifier', 'humidifier moist', 'ict', 'ict 2002', 'id', 'id 10299452874797tmda', 'idea', 'idea long', 'ideal', 'ideal patriotic', 'imply', 'imply guilt', 'inbox', 'inbox list', 'including', 'including clothes', 'independence', 'independence visit', 'inducing', 'inducing judgment', 'information', 'information go', 'information url', 'int', 'int note', 'interest', 'interest rates', 'invest', 'invest less', 'investing', 'investing would', 'issue', 'issue ask', 'issue think', 'january', 'january 2002', 'join', 'join url', 'journey', 'journey financial', 'judged', 'judged limestone', 'judgment', 'judgment live', 'junk', 'junk mail', 'keep', 'keep let', 'keep market', 'keep motley', 'keep resolutions', 'keep short', 'keep turning', 'keep url', 'kerdylio', 'kerdylio 70', 'kid', 'kid tuition', 'kre', 'kre ps', 'law', 'law changes', 'lbrace', 'lbrace lbrace', 'lbrace subject', 'least', 'least one', 'leave', 'leave hanging', 'leave house', 'left', 'left coupons', 'legal', 'legal information', 'less', 'less hour', 'less make', 'let', 'let face', 'let get', 'like', 'like 10', 'like dark', 'like every', 'limestone', 'limestone granite', 'limestone mount', 'limestone weather', 'line', 'line search', 'list', 'list actually', 'list exmh', 'list lbrace', 'list seems', 'list since', 'list url', 'lists', 'lists honor', 'lists telemarketing', 'little', 'little stuff', 'live', 'live comfortably', 'live every', 'living', 'living means', 'local', 'local routing', 'log', 'log pick', 'long', 'long distance', 'long term', 'lose', 'lose lot', 'lot', 'lot weight', 'lower', 'lower credit', 'mail', 'mail already', 'mail er', 'mail lists', 'mail offering', 'mailer', 'mailer sender', 'mailing', 'mailing list', 'main', 'main credit', 'make', 'make least', 'make sure', 'many', 'many credit', 'mar', 'mar 17', 'march', 'march 10', 'market', 'market course', 'market short', 'market um', 'market url', 'market volatile', 'markets', 'markets url', 'marking', 'marking hits', 'martin', 'martin posted', 'masterv', 'masterv 184', 'mattress', 'mattress earn', 'meal', 'meal week', 'means', 'means discussion', 'mediation', 'mediation ten', 'mercury', 'mercury 18', 'mercury hit', 'message', 'message id', 'message recipient', 'mh_profile', 'mh_profile delta', 'mh_profile get', 'mhparam', 'mhparam pick', 'mileage', 'mileage go', 'miles', 'miles 310', 'miles east', 'miles mileage', 'mkettler', 'mkettler homecom', 'moist', 'moist air', 'moment', 'moment good', 'monastic', 'monastic community', 'money', 'money advisor', 'money going', 'money left', 'money markets', 'money resolutions', 'money saving', 'money stock', 'money url', 'month', 'month trial', 'mostly', 'mostly fail', 'mostly nightmares', 'motley', 'motley fool', 'mount', 'mount athos', 'mount kerdylio', 'mountain', 'mountain limestone', 'msg', 'msg 7678', 'msgid', 'msgid msg', 'much', 'much compare', 'museum', 'museum restored', 'nasdaq', 'nasdaq height', 'new', 'new crash', 'new one', 'new ones', 'new sequences', 'new year', 'nifty', 'nifty savings', 'night', 'night leave', 'nightmares', 'nightmares hang', 'nmh', 'nmh 104', 'nmh using', 'note', 'note run', 'obviously', 'obviously version', 'offering', 'offering free', 'offering new', 'offers', 'offers mail', 'offers url', 'one', 'one 60', 'one asks', 'one comes', 'one crock', 'one explicit', 'one month', 'one phone', 'one thing', 'one today', 'ones', 'ones stop', 'online', 'online built', 'online equity', 'opportunity', 'opportunity save', 'order', 'order get', 'owes', 'owes bad', 'papadopoulos', 'papadopoulos greek', 'park', 'park admiring', 'part', 'part mh_profile', 'patriotic', 'patriotic sculpture', 'payment', 'payment kid', 'perks', 'perks fools', 'person', 'person lose', 'personal', 'personal finance', 'phone', 'phone call', 'phone calls', 'phone lists', 'pick', 'pick command', 'pick happening', 'pick inbox', 'pick nmh', 'pick seq', 'pick version', 'pick_it', 'pick_it exec', 'pitching', 'pitching retirement', 'place', 'place keep', 'plan', 'plan document', 'plan judged', 'planned', 'planned mountain', 'popup', 'popup one', 'portfolio', 'portfolio url', 'posted', 'posted tassos', 'pot', 'pot meal', 'pressure', 'pressure right', 'pretty', 'pretty fast', 'proactive', 'proactive list', 'problems', 'problems url', 'professional', 'professional advice', 'proprietary', 'proprietary auto', 'ps', 'ps still', 'purse', 'purse find', 'put', 'put together', 'quick', 'quick proactive', 'rates', 'rates drop', 'rates hire', 'rates one', 'rbrace', 'rbrace 4852', 'rbrace rbrace', 'reach', 'reach cvs', 'really', 'really speculating', 'recent', 'recent tax', 'recipient', 'recipient mkettler', 'redhatcom', 'redhatcom url', 'refinance', 'refinance save', 'reimbursed', 'reimbursed every', 'relevant', 'relevant part', 'removed', 'removed junk', 'repeatable', 'repeatable like', 'report', 'report going', 'reports', 'reports little', 'repository', 'repository today', 'reproduce', 'reproduce error', 'request', 'request full', 'reserved', 'reserved legal', 'resolution', 'resolution get', 'resolution save', 'resolutions', 'resolutions keep', 'resolutions mostly', 'resolutions vague', 'restored', 'restored amphitheatre', 'results', 'results less', 'retire', 'retire early', 'retirement', 'retirement accounts', 'right', 'right wrong', 'rights', 'rights reserved', 'routing', 'routing issue', 'routing technology', 'run', 'run humidifier', 'run pick', 'salonika', 'salonika far', 'save', 'save big', 'save bunch', 'save interest', 'save money', 'saving', 'saving tips', 'savings', 'savings center', 'savings keep', 'savings market', 'savings mattress', 'savings stash', 'savings visit', 'sculptor', 'sculptor behind', 'sculpture', 'sculpture well', 'search', 'search popup', 'second', 'second execution', 'seconds', 'seconds url', 'seems', 'seems daunting', 'sel', 'sel list', 'send', 'send email', 'sender', 'sender daemonv', 'sender masterv', 'seq', 'seq sel', 'sequence', 'sequence actually', 'sequence mercury', 'sequences', 'sequences window', 'service', 'service ask', 'service offering', 'settings', 'settings url', 'sharing', 'sharing least', 'short', 'short term', 'shortcut', 'shortcut check', 'since', 'since pick', 'solicitation', 'solicitation url', 'someone', 'someone contacts', 'something', 'something eat', 'special', 'special offers', 'speculating', 'speculating investing', 'sponsor', 'sponsor dvds', 'sponsored', 'sponsored datek', 'spotlight', 'spotlight savings', 'staff', 'staff help', 'start', 'start pitching', 'started', 'started dial', 'stash', 'stash short', 'states', 'states agreed', 'still', 'still using', 'stock', 'stock market', 'stocks', 'stocks short', 'stop', 'stop solicitation', 'stop three', 'store', 'store buying', 'stuff', 'stuff could', 'stuff like', 'stuff special', 'subject', 'subject ftp', 'subject url', 'subscription', 'subscription information', 'sun', 'sun mar', 'supporters', 'supporters idea', 'sure', 'sure broker', 'sure carry', 'sure get', 'syntax', 'syntax error', 'take', 'take shortcut', 'tassos', 'tassos papadopoulos', 'tax', 'tax law', 'taxes', 'taxes keep', 'technology', 'technology 999', 'telemarketing', 'telemarketing phone', 'tempted', 'tempted go', 'ten', 'ten miles', 'term', 'term cash', 'term gain', 'term market', 'term money', 'term savings', 'thanks', 'thanks recent', 'thermostat', 'thermostat night', 'thing', 'thing differently', 'thing going', 'think', 'think _______________________________________________', 'three', 'three main', 'time', 'time either', 'time without', 'times', 'times year', 'tips', 'tips add', 'tips aid', 'tips least', 'tired', 'tired getting', 'tkerror', 'tkerror syntax', 'tmf', 'tmf money', 'today', 'today local', 'today url', 'together', 'together quick', 'trade', 'trade proprietary', 'trades', 'trades 60', 'trial', 'trial subscription', 'true', 'true lower', 'try', 'try one', 'tuition', 'tuition nasdaq', 'turning', 'turning thermostat', 'um', 'um big', 'unbiased', 'unbiased professional', 'united', 'united states', 'unsubscribe', 'unsubscribe change', 'unsubscribe egroupscom', 'unsubscribe group', 'unsubscribe unsubscribe', 'url', 'url _____________________________________________________', 'url ask', 'url community', 'url customer', 'url despair', 'url discussion', 'url get', 'url invest', 'url list', 'url money', 'url msgid', 'url perks', 'url portfolio', 'url refinance', 'url retire', 'url sponsored', 'url spotlight', 'url sun', 'url unsubscribe', 'url wanna', 'use', 'use yahoo', 'using', 'using delta', 'using version', 'vague', 'vague imply', 'value', 'value savings', 'version', 'version code', 'version mailer', 'version nmh', 'version pick', 'visit', 'visit nifty', 'visit personal', 'volatile', 'volatile buying', 'wait', 'wait 30', 'wanna', 'wanna take', 'want', 'want go', 'want mail', 'wanted', 'wanted emergency', 'way', 'way get', 'weather', 'weather pretty', 'wed', 'wed 21', 'wednesday', 'wednesday january', 'week', 'week get', 'weight', 'weight pressure', 'well', 'well alexander', 'wide', 'wide museum', 'window', 'window date', 'without', 'without fail', 'workers', 'workers mailing', 'workers redhatcom', 'works', 'works sequence', 'worth', 'worth filing', 'would', 'would great', 'would wanted', 'wrong', 'wrong url', 'yahoo', 'yahoo groups', 'year', 'year 30', 'year resolution', 'year resolutions', 'yields', 'yields cds', 'zzzzteana', 'zzzzteana alexander']\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(stop_words=stopwords.words('english'), ngram_range=(1, 2))\n",
    "print(vectorizer.fit_transform(X_few_text))\n",
    "print(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(ham_emails + spam_emails)\n",
    "y = np.array([0] * len(ham_emails) + [1] * len(spam_emails))\n",
    "\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "email_pipeline = Pipeline([\n",
    "    (\"EmailText to Words\", EmailTextToWords()),\n",
    "    (\"Words to Count Vector\", CountVectorizer(stop_words=stopwords.words('english'), ngram_range=(1, 2))),\n",
    "])\n",
    "\n",
    "X_augmented_train = email_pipeline.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9945147354298663"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_clf = LogisticRegression(solver=\"liblinear\", random_state=42)\n",
    "score = model_selection.cross_val_score(log_clf, X_augmented_train, y_train, cv=3)\n",
    "score.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 97.44%\n",
      "Recall: 100.00%\n"
     ]
    }
   ],
   "source": [
    "X_augmented_test = email_pipeline.transform(X_test)\n",
    "\n",
    "log_clf = LogisticRegression(solver=\"liblinear\", random_state=42)\n",
    "log_clf.fit(X_augmented_train, y_train)\n",
    "\n",
    "y_pred = log_clf.predict(X_augmented_test)\n",
    "\n",
    "print(\"Precision: {:.2f}%\".format(100 * precision_score(y_test, y_pred)))\n",
    "print(\"Recall: {:.2f}%\".format(100 * recall_score(y_test, y_pred)))\n",
    "\n",
    "# just hard_ham got:\n",
    "# Precision: 97.12%\n",
    "# Recall: 99.26%\n",
    "\n",
    "# from joblib import dump, load\n",
    "# dump(log_clf, 'filename.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "# pre-read for rnn stuff: https://www.analyticsvidhya.com/blog/2020/03/pretrained-word-embeddings-nlp\n",
    "\n",
    "# load the whole embedding into memory\n",
    "embeddings_index = dict()\n",
    "f = open('data/glove.6B/glove.6B.300d.txt')\n",
    "\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "\n",
    "f.close()\n",
    "print('Loaded %s word vectors.' % len(embeddings_index)) # 400,000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2734,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'i\\'m on tv tonight\\nurl: URL\\ndate: not supplied\\n\\ntonight on the style network\\'s tv show \"area\" my house will be featured \\nundergoing a hawaiiana makeover watch it and meet carla my daughter and me \\nit\\'ll play monday at 9:30 pm et (if someone can tape it for me i\\'d appreciate \\nit because my cable service doesn\\'t get the style channel i\\'ll send you a new \\nt-shirt iron on of a girl and her pet slug email mark@wellcom) link[1] \\ndiscuss[2]\\n\\n[1] URL\\n[2] URL\\n\\n\\n'"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_for_rnn = EmailTextToWords().fit_transform(X_train)\n",
    "print(X_train_for_rnn.shape)\n",
    "X_train_for_rnn[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(684,)"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_for_rnn = EmailTextToWords().fit_transform(X_test)\n",
    "X_test_for_rnn.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,   133,    15,  1254,  1704,\n",
       "           6,     6,   126,    25,   515,  1704,    15,     1,  1726,\n",
       "       20318,  1254,   596,   881,    39,  1050,    33,    19,  3949,\n",
       "       11747,     4, 20319, 14539,  1271,    14,     3,   334, 20320,\n",
       "          39,  4710,     3,    52,  3486,   281,  1076,    21,   361,\n",
       "         529,   539,   731,    24,   434,    28,  4711,    14,     9,\n",
       "          52,   442,  3580,    14,   148,    39,  2142,   369,   343,\n",
       "          34,     1,  1726,  3118,   462,   321,     8,     4,    49,\n",
       "         327,  5441,  7179,    15,     5,     4,  1727,     3,   322,\n",
       "        7180, 20321,    45,   787, 20322,   159,    55,   675,    90,\n",
       "          55,     6,    90,     6], dtype=int32)"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# tokenize the sentences\n",
    "tokenizer = Tokenizer()\n",
    "\n",
    "# preparing vocabulary\n",
    "tokenizer.fit_on_texts(list(X_train_for_rnn))\n",
    "\n",
    "# converting text into integer sequences\n",
    "x_train_seq  = tokenizer.texts_to_sequences(X_train_for_rnn) \n",
    "x_test_seq = tokenizer.texts_to_sequences(X_test_for_rnn)\n",
    "\n",
    "# padding to prepare sequences of same length\n",
    "x_train_seq  = pad_sequences(x_train_seq, maxlen=400)\n",
    "x_test_seq = pad_sequences(x_test_seq, maxlen=400)\n",
    "\n",
    "x_train_seq[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37238"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "size_of_vocabulary=len(tokenizer.word_index) + 1\n",
    "size_of_vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 0.04656   ,  0.21318001, -0.0074364 , ...,  0.0090611 ,\n",
       "        -0.20988999,  0.053913  ],\n",
       "       [-0.25756001, -0.057132  , -0.67189997, ..., -0.16043   ,\n",
       "         0.046744  , -0.070621  ],\n",
       "       ...,\n",
       "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 0.15554   , -0.16915999,  0.24866   , ...,  0.36546001,\n",
       "         0.53972   ,  0.40272999],\n",
       "       [ 0.21274   , -0.21021999, -0.23616999, ...,  0.32126001,\n",
       "         0.19767   , -0.059769  ]])"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a weight matrix for words in training docs\n",
    "embedding_matrix = np.zeros((size_of_vocabulary, 300)) # 300 b/c our glove is 300 dimensions\n",
    "\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        \n",
    "embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 400, 300)          11171400  \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 400, 128)          219648    \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_1 (Glob (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 11,399,369\n",
      "Trainable params: 227,969\n",
      "Non-trainable params: 11,171,400\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import *\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.callbacks import *\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# embedding layer - https://keras.io/api/layers/core_layers/embedding/\n",
    "model.add(Embedding(size_of_vocabulary, 300, weights=[embedding_matrix], input_length=400, trainable=False)) \n",
    "\n",
    "# lstm layer - https://keras.io/api/layers/recurrent_layers/lstm/\n",
    "model.add(LSTM(128, return_sequences=True, dropout=0.2))\n",
    "\n",
    "# global Maxpooling\n",
    "model.add(GlobalMaxPooling1D())\n",
    "\n",
    "# dense Layer\n",
    "model.add(Dense(64, activation='relu')) \n",
    "model.add(Dense(1, activation='sigmoid')) \n",
    "\n",
    "# add loss function, metrics, optimizer\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=[\"acc\"]) \n",
    "\n",
    "# adding callbacks\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=3)  \n",
    "mc = ModelCheckpoint('best_model.h5', monitor='val_acc', mode='max', save_best_only=True, verbose=1)  \n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.4238 - acc: 0.7886\n",
      "Epoch 00001: val_acc improved from -inf to 0.89620, saving model to best_model.h5\n",
      "22/22 [==============================] - 20s 888ms/step - loss: 0.4238 - acc: 0.7886 - val_loss: 0.2664 - val_acc: 0.8962\n",
      "Epoch 2/10\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.1963 - acc: 0.9323\n",
      "Epoch 00002: val_acc improved from 0.89620 to 0.98538, saving model to best_model.h5\n",
      "22/22 [==============================] - 18s 805ms/step - loss: 0.1963 - acc: 0.9323 - val_loss: 0.0689 - val_acc: 0.9854\n",
      "Epoch 3/10\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.0618 - acc: 0.9828\n",
      "Epoch 00003: val_acc improved from 0.98538 to 0.99561, saving model to best_model.h5\n",
      "22/22 [==============================] - 19s 871ms/step - loss: 0.0618 - acc: 0.9828 - val_loss: 0.0244 - val_acc: 0.9956\n",
      "Epoch 4/10\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.0248 - acc: 0.9931\n",
      "Epoch 00004: val_acc did not improve from 0.99561\n",
      "22/22 [==============================] - 18s 811ms/step - loss: 0.0248 - acc: 0.9931 - val_loss: 0.0141 - val_acc: 0.9956\n",
      "Epoch 5/10\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.0185 - acc: 0.9938\n",
      "Epoch 00005: val_acc improved from 0.99561 to 0.99854, saving model to best_model.h5\n",
      "22/22 [==============================] - 18s 815ms/step - loss: 0.0185 - acc: 0.9938 - val_loss: 0.0092 - val_acc: 0.9985\n",
      "Epoch 6/10\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.0076 - acc: 0.9982\n",
      "Epoch 00006: val_acc did not improve from 0.99854\n",
      "22/22 [==============================] - 18s 798ms/step - loss: 0.0076 - acc: 0.9982 - val_loss: 0.0045 - val_acc: 0.9985\n",
      "Epoch 7/10\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.0045 - acc: 0.9993\n",
      "Epoch 00007: val_acc improved from 0.99854 to 1.00000, saving model to best_model.h5\n",
      "22/22 [==============================] - 17s 777ms/step - loss: 0.0045 - acc: 0.9993 - val_loss: 0.0065 - val_acc: 1.0000\n",
      "Epoch 8/10\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.0024 - acc: 1.0000\n",
      "Epoch 00008: val_acc did not improve from 1.00000\n",
      "22/22 [==============================] - 17s 779ms/step - loss: 0.0024 - acc: 1.0000 - val_loss: 0.0015 - val_acc: 1.0000\n",
      "Epoch 9/10\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.0014 - acc: 1.0000\n",
      "Epoch 00009: val_acc did not improve from 1.00000\n",
      "22/22 [==============================] - 17s 785ms/step - loss: 0.0014 - acc: 1.0000 - val_loss: 0.0015 - val_acc: 1.0000\n",
      "Epoch 10/10\n",
      "22/22 [==============================] - ETA: 0s - loss: 0.0017 - acc: 0.9996\n",
      "Epoch 00010: val_acc did not improve from 1.00000\n",
      "22/22 [==============================] - 18s 827ms/step - loss: 0.0017 - acc: 0.9996 - val_loss: 0.0010 - val_acc: 1.0000\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    np.array(x_train_seq),\n",
    "    np.array(y_train),\n",
    "    batch_size=128,\n",
    "    epochs=10,\n",
    "    validation_data=(np.array(x_test_seq), np.array(y_test)),\n",
    "    verbose=1,\n",
    "    callbacks=[es,mc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 1s 226ms/step - loss: 0.0065 - acc: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load best model\n",
    "from tensorflow.keras.models import load_model\n",
    "model = load_model('best_model.h5')\n",
    "\n",
    "# evaluation \n",
    "_, val_acc = model.evaluate(x_test_seq, y_test, batch_size=128)\n",
    "val_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nazario_spam_filenames = [name for name in sorted(os.listdir('data/phishing-2019')) if len(name) > 3]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
